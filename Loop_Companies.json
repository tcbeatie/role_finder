{
  "name": "Loop Companies v3.1",
  "nodes": [
    {
      "parameters": {
        "inputSource": "passthrough"
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        0,
        0
      ],
      "id": "23343072-e0dd-48fb-b645-ab4a62c54d97",
      "name": "Start",
      "notes": "═════════════════════════\n     NODE 1: WORKFLOW TRIGGER + PROFILE LOADER     ═════════════════════════\n     Workflow Entry Point - This is where the Company\n     workflow starts!\n\n     What it does:\n     - Starts the workflow\n     - No configuration or parameters needed in the\n     node itself\n     - Provides PROFILE DATA through trigger execution\n     context\n\n     Profile data structure expected:\n     {\n     profile_id: string,           // Id\n     profile_name: string,          // Name\n     resume_text: string,           // Full resume/experience text\n     target_criteria: string,       // JSON string with job search criteria\n     notes: string                  // Additional profile notes\n     }\n\n     How profile flows through workflow:\n     1. Start trigger provides profile data\n     2. Merge Data combines profile with each company\n     3. Build Request extracts criteria from\n     profile.target_criteria\n     4. Add Context passes profile fields to Loop Jobs\n     sub-workflow\n     5. Loop Jobs uses resume_text for AI evaluation\n\n     Next nodes:\n     - Load Companies (loads target companies in\n     parallel)\n     - Merge Data (receives profile from this node)"
    },
    {
      "parameters": {
        "operation": "get",
        "dataTableId": {
          "__rl": true,
          "value": "7wfHhDMT9m1gvY1a",
          "mode": "list",
          "cachedResultName": "Companies",
          "cachedResultUrl": "/projects/NRVzB7ls7lOjZ60v/datatables/7wfHhDMT9m1gvY1a"
        },
        "returnAll": true
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1.1,
      "position": [
        208,
        -160
      ],
      "id": "1bda3e9f-45bb-49c2-8d01-aca5b8b9040c",
      "name": "Load Companies",
      "notes": "═════════════════════════\n     NODE 2: LOAD COMPANIES TO MONITOR     ═════════════════════════\n     Loads target companies from n8n data table.\n\n     What it does:\n     - Connects to n8n data table called 'companies'\n     - Loads ALL rows (returnAll: true means no limit)\n     - Each row represents one company to monitor for jobs\n\n     Expected table columns:\n     - company_id (string): Unique identifier (e.g.,\n     \"anthropic\", \"openai\")\n     - company_name (string): Display name (e.g.,\n     \"Anthropic\", \"OpenAI\")\n     - domain (string): Company website domain for job\n     search (e.g., \"anthropic.com\")\n\n     Example data:\n     [\n     { company_id: \"anthropic\", company_name:\n     \"Anthropic\", domain: \"anthropic.com\" },\n     { company_id: \"openai\", company_name: \"OpenAI\",\n     domain: \"openai.com\" }\n     ]\n\n     Output: Array of company objects, one per table\n     row\n\n     Production notes:\n     - For testing: Use 1-3 companies to validate\n     workflow\n     - For production: Can handle 40+ companies\n     efficiently\n     - Companies processed sequentially (fault\n     tolerance)\n\n     Architectural note:\n     This node loads companies in PARALLEL with profile\n     loading at Start trigger.\n     Both inputs merge in the Merge Data node before\n     the company loop begins.\n\n     Next node: Merge Data (combines with profile)"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// MERGE COMPANIES WITH PROFILE\n// =====================================================================\n\n// Get companies by referencing the node NAME\nconst companies = $('Load Companies').all();\n\n// Get profile by referencing the node NAME  \nconst profile = $('Start').first().json;\n\nconsole.log('Companies:', companies.length);\nconsole.log('Profile ID:', profile.profile_id);\n\n// Create merged output\nreturn companies.map(item => ({\n  json: {\n    // Company fields\n    company_id: item.json.company_id,\n    company_name: item.json.company_name,\n    domain: item.json.domain,\n    \n    // Profile fields\n    profile_id: profile.profile_id,\n    profile_name: profile.profile_name,\n    resume_text: profile.resume_text,\n    target_criteria: profile.target_criteria,\n    notes: profile.notes\n  }\n}));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        416,
        0
      ],
      "id": "6f60d909-b5ac-4253-85c9-30397a9cff19",
      "name": "Merge Data",
      "notes": "=========================================================================\n     NODE 3: MERGE COMPANIES WITH PROFILE DATA\n     Purpose: Combines company list with candidate profile for processing\n     Execution Mode: Run Once for All Items     =========================================================================\n     ARCHITECTURAL SIGNIFICANCE:\n     This is where profile externalization happens.\n     Instead of hardcoding profile data in the workflow, we load it once at the top\n     and merge it with each company. This creates a \"company + profile\"\n     context that flows through the entire workflow.\n\n     STEP 1: Get companies by referencing the Load Companies node\n     .all() returns array of ALL items from that node\n     const companies = $('Load Companies').all();\n\n     STEP 2: Get profile by referencing the Start trigger node\n     .first() gets the first (and only) item from Start trigger\n     The profile data is provided via trigger execution context\n     const profile = $('Start').first().json;\n\n     DEBUGGING: Log what we received (visible in execution log)\n     console.log('Companies loaded:', companies.length);\n     console.log('Profile ID:', profile.profile_id);\n\n     STEP 3: Create merged output - one item per company, each with profile data\n     .map() transforms each company into a combined company+profile object\n     return companies.map(item => ({\n       json: {\n         // === COMPANY FIELDS ===\n     // These identify which company we're processing\n     company_id: item.json.company_id,      // \"anthropic\"\n     company_name: item.json.company_name,  //  \"Anthropic\"\n     domain: item.json.domain,              //  \"anthropic.com\"\n\n         // === PROFILE FIELDS ===\n     // These provide candidate context for job matching\n     profile_id: profile.profile_id,              // \"[id]\"\n     profile_name: profile.profile_name,          //  \"[name]\"\n     resume_text: profile.resume_text,            // Full resume/experience\n     target_criteria: profile.target_criteria,    //  JSON string with search criteria\n     notes: profile.notes                         //   Additional notes (optional)\n       }\n     }));\n\n     OUTPUT: Array of merged objects, one per company\n     Example for 3 companies:\n     [\n     { company_id: \"anthropic\", domain: \"anthropic.com\", profile_id: \"name\", resume_text: \"...\", ... },\n     { company_id: \"openai\", domain: \"openai.com\", profile_id: \"name\", resume_text: \"...\", ... },\n     { company_id: \"google\", domain: \"google.com\", profile_id: \"name\", resume_text: \"...\", ... }\n     ]\n\n     WHY MERGE HERE?\n     1. LOAD ONCE: Profile loaded once at top, not in loop (efficient)\n     2. CONTEXT PRESERVATION: Every company carries full profile context\n     3. CLEAN ARCHITECTURE: Loop Over Companies\n     receives complete context per iteration\n     4. SIMPLIFIES DOWNSTREAM: No need to reference profile node in loop\n\n     DATA FLOW:\n     This merged data flows to:\n     1. Build Request → Extracts target_criteria for Apify search\n     2. Loop Over Companies → Each iteration has company + profile\n     3. Add Context → Passes profile fields to sub-workflow\n     4. Loop Jobs → Uses resume_text for AI evaluation\n\n     Next node: Build Request"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE: BUILD APIFY REQUEST\n// =====================================================================\n\nconst items = $input.all();\n\nreturn items.map(item => {\n  const data = item.json;\n  \n  // Parse the target criteria from the profile data (now included in each item)\n  const criteria = JSON.parse(data.target_criteria);\n  \n  return {\n    json: {\n      // === APIFY SEARCH PARAMETERS ===\n      domainFilter: [data.domain],\n      limit: 10,\n      timeRange: \"24h\",\n      titleSearch: criteria.titleSearch,\n      titleExclusionSearch: criteria.titleExclusionSearch,\n      locationSearch: [\"United States\", \"Canada\", \"United Kingdom\", \"Ireland\", \"Netherlands\", \"Germany\"],\n      aiWorkArrangementFilter: [\"Hybrid\", \"Remote OK\", \"Remote Solely\"],\n      \n      // Additional Apify features\n      includeAi: true,\n      includeLinkedIn: true,\n      populateAiRemoteLocationDerived: true,\n      descriptionType: \"text\"\n    }\n  };\n});"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        624,
        0
      ],
      "id": "c4e13530-7186-4a60-8804-a71b3fcfbde4",
      "name": "Build Request",
      "notes": "=========================================================================\n     NODE 4: BUILD APIFY REQUEST FROM MERGED DATA\n     Purpose: Transform company+profile data into Apify\n     API request format\n     Execution Mode: Run Once for All Items     =========================================================================\n     STEP 1: Get all merged company+profile items\n     const items = $input.all();\n\n     STEP 2: Transform each item into Apify API request format\n     return items.map(item => {\n       const data = item.json;\n\n       // Parse the target criteria from profile data\n     // target_criteria is stored as JSON string in profile, so we parse it\n       // Expected structure:\n       // {\n       //   titleSearch: [\"product manager\"],\n     //   titleExclusionSearch: [\"product marketing\"],\n       //   ... other criteria ...\n       // }\n     const criteria = JSON.parse(data.target_criteria);\n\n       return {\n         json: {\n           // === APIFY API SEARCH PARAMETERS ===\n           // These fields tell Apify what jobs to find\n\n           domainFilter: [data.domain],\n     // Company website to search (must be array format)\n           // Example: [\"anthropic.com\"]\n\n           limit: 10,\n     // Maximum number of jobs to return per company\n     // Increase for more jobs, decrease for faster/cheaper searches\n\n           timeRange: \"7d\",\n           // How far back to search for jobs\n     // Options: \"1h\", \"24h\", \"7d\", \"30d\", \"3m\", \"6m\"\n           // PRODUCTION: \"7d\" for weekly, or \"24h\" for daily\n           // TESTING: \"6m\" to ensure jobs are found\n\n           titleSearch: criteria.titleSearch,\n     // EXTRACTED FROM PROFILE: Only return jobs with these words in title\n     // Example: [\"product manager\"] from profile\n     target_criteria\n           // Case-insensitive search\n\n     titleExclusionSearch:\n     criteria.titleExclusionSearch,\n     // EXTRACTED FROM PROFILE: Exclude jobs with these words in title\n     // Example: [\"product marketing\"] from profile\n     target_criteria\n     // Helps filter out Product Marketing Manager roles\n\n     aiWorkArrangementFilter: [\"Hybrid\", \"Remote OK\", \"Remote Solely\"],\n     // Only return jobs that are Remote or Hybrid\n           // Filters out on-site only positions\n     // Note: Could be moved to profile target_criteria in future\n\n           // === APIFY FEATURE FLAGS ===\n           includeAi: true,\n     // Enable Apify's AI analysis (extracts salary, experience level, etc.)\n\n           includeLinkedIn: true,\n     // Search LinkedIn jobs in addition to company career pages\n\n           populateAiRemoteLocationDerived: true,\n     // Extract and standardize remote work locations\n\n           descriptionType: \"text\"\n     // Return job descriptions as plain text (alternative: \"html\")\n\n     // NOTE: We do NOT include _context_ fields here\n     // Those are added later by the \"Add Context\" node\n     // This keeps the Apify request clean and focused on search parameters\n         }\n       };\n     });\n\n     WHAT GETS OUTPUT:\n     Array of Apify request objects, one per company\n     Example for 2 companies:\n     [\n     { domainFilter: [\"anthropic.com\"], limit: 10, titleSearch: [\"product manager\"], ... },\n     { domainFilter: [\"openai.com\"], limit: 10, titleSearch: [\"product manager\"], ... }\n     ]\n\n     Next node: Loop Over Companies"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        832,
        0
      ],
      "id": "6a09c3b3-3210-499b-9793-f159c9a15bda",
      "name": "Loop Over Companies",
      "notes": " ═════════════════════════\n     NODE 5: LOOP OVER COMPANIES (COMPANY WORKFLOW  ORCHESTRATOR)     ═════════════════════════\n     - Takes an array of items (companies) and\n     processes them one by one\n     - After processing one company completely,\n     automatically advances to next\n     - Batch size: 1 (default) means process one\n     company per loop iteration\n\n     Why process one company at a time instead of all\n     at once?\n     1. FAULT TOLERANCE: If company #5 fails, companies #6-40 still process\n     2. INCREMENTAL SAVES: Results saved to database after each company\n     3. PROGRESS TRACKING: Can monitor real-time\n     progress in n8n UI\n     4. COST CONTROL: Know exact API call counts and costs\n     5. CLEAN SEPARATION: Each company's jobs are\n     processed as a complete unit\n     before moving to the next company\n\n     How the loop works:\n     ┌─ Loop iteration 1: Process Company 1 (Anthropic)\n     │  ├─ Call Apify → Get 8 jobs\n     │  ├─ Save raw jobs to database (fast path)\n     │  ├─ Call Loop Jobs sub-workflow → Process all 8 jobs (slow path)\n     │  └─ Return to this loop node\n     │\n     ├─ Loop iteration 2: Process Company 2 (OpenAI)\n     │  ├─ Call Apify → Get 5 jobs\n     │  ├─ Save raw jobs to database (fast path)\n     │  ├─ Call Loop Jobs sub-workflow → Process all 5 jobs (slow path)\n     │  └─ Return to this loop node\n     │\n     └─ All companies done → Done branch activates\n\n     Two outputs (pins on the node):\n     1. DONE BRANCH (top pin):\n     - Fires ONCE after ALL companies complete\n     - Routes to: Workflow Summary\n     - Used for triggering email workflow\n\n     2. LOOP BRANCH (bottom pin):\n     - Fires ONCE PER COMPANY\n     - Routes to: Run Apify\n     - This is where the work happens\n\n     Accessing current company in downstream nodes:\n     $('Loop Over Companies').item.json\n     // This gives you the current company being\n     processed\n     // Includes BOTH company fields AND profile fields\n     from merge:\n     // {\n     //   company_id: \"anthropic\",\n     //   company_name: \"Anthropic\",\n     //   domain: \"anthropic.com\",\n     //   profile_id: \"[id]\",\n     //   resume_text: \"...\",\n     //   target_criteria: \"{...}\",\n     //   ...\n     // }\n\n     Context preservation:\n     Each loop iteration has access to the merged\n     company+profile data.\n     This means downstream nodes (Add Context, Save\n     Apify data, etc.) can\n     reference both company fields and profile fields\n     without additional lookups.\n\n     Next nodes:\n     - Loop branch → Run Apify (process current\n     company)\n     - Done branch → Workflow Summary (prepare for\n     email workflow)\n"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE: PREPARE FOR EMAIL QUERY\n// =====================================================================\n\n// The Done branch receives multiple items (one per job processed)\n// We only need to query the database ONCE using the workflow run ID\n// This node consolidates to a single item\n\nreturn [{\n  json: {\n    workflow_run_id: $execution.id,\n    jobs_processed: $input.all().length,\n    query_timestamp: new Date().toISOString()\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1040,
        -96
      ],
      "id": "099ced12-16f5-4450-8ac0-98e80e39255f",
      "name": "Workflow Summary",
      "notes": " ========================================================================\n     NODE 15: PREPARE FOR EMAIL QUERY\n     Purpose: Convert multiple loop items into single\n     execution context     =========================================================================\n     THE PROBLEM:\n     The Done branch from Loop Over Companies receives multiple items\n     (one item per company processed). Each item is the\n     merged company+profile data.\n     But we only need to query the database ONCE using the workflow run ID.\n\n     THE SOLUTION:\n     Consolidate all those items into a SINGLE item with summary info.\n     This prevents the email workflow from being called multiple times.\n\n     Get all items that came through the loop (all companies)\n     const allCompanies = $input.all();\n\n     Create a single summary item\n     return [{\n       json: {\n         // Workflow identifier for email query\n         workflow_run_id: $execution.id,\n\n         // Summary statistics\n         companies_processed: allCompanies.length,\n\n     // Profile info (from first company - all have same profile)\n     profile_id: allCompanies[0]?.json.profile_id || 'unknown',\n     profile_name: allCompanies[0]?.json.profile_name || 'Unknown',\n\n         // Timestamp\n         query_timestamp: new Date().toISOString()\n       }\n     }];\n\n     WHAT GETS OUTPUT:\n     Single item with:\n     - workflow_run_id: Use this to query email queue\n     - companies_processed: How many companies we checked\n     - profile_id: Which profile was used for this run\n     - query_timestamp: When this summary was created\n\n     - Email workflow executes ONCE\n     - Returns all jobs from this workflow run\n     - User receives single consolidated email\n\n     Next: This data gets sent to the Send Email workflow from Main\n"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 3
          },
          "conditions": [
            {
              "id": "c8b41d35-a242-47aa-87a5-35d04194569d",
              "leftValue": "={{ !!($json && ($json.id || $json.title || $json.organization)) }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.3,
      "position": [
        1248,
        0
      ],
      "id": "1ffa4b32-5f94-4d35-9bfc-7e65ca89f0b0",
      "name": "If",
      "notes": " ════════════════════════════════════════════════\n     NODE 7: CHECK IF JOBS WERE FOUND     ════════════════════════════════════════════════\n     Determines if Apify returned actual job data or empty results.\n\n     The problem we're solving:\n     - When Apify finds 0 jobs, it might return empty items or no items\n     - We need to distinguish between \"has jobs\" vs \"no jobs\" to route correctly\n\n     Condition explained in detail:\n     {{ !!($json && ($json.id || $json.title || $json.organization)) }}\n\n     Let's break this down:\n\n     1. $json\n     - The current item from Run Apify node\n     - Could be a real job OR an empty item\n\n     2. $json.id || $json.title || $json.organization\n     - Checks if ANY of these fields exist\n     - Real jobs always have at least one of these\n     - Empty items have none\n     - Returns: the field value (truthy) OR undefined (falsy)\n\n     3. !!\n     - Double negation - converts ANY value to boolean (true/false)\n     - Why? Because IF node needs true/false, not a string or undefined\n     - \"1234\" → !!\"1234\" → true\n     - undefined → !!undefined → false\n\n     Examples:\n     - Real job: {id: \"1234\", title: \"Product Manager\", ...} → TRUE\n     - Empty item: {} → FALSE\n     - Missing item: → FALSE\n\n     1. TRUE BRANCH (jobs found):\n     Routes to: Add Context node\n\n     Add Context then splits into TWO PARALLEL paths:\n\n     Path A: Save Apify data → Insert row (ends cleanly)\n     - Saves raw job data to database immediately\n     - Fast path - completes in ~100ms per job\n     - Does NOT loop back or call sub-workflow\n     - Purpose: Backup and searchable job storage\n\n     Path B: Call 'Loop Jobs' sub-workflow → loops back to company loop\n     - Calls separate \"Loop Jobs\" workflow to process each job\n     - That workflow handles: AI evaluation, merge, format, email queue\n     - Slow path - takes 2-5 seconds per job\n     - DOES loop back when ALL jobs for current company complete\n     - Purpose: AI-powered job matching and email preparation\n\n     WHY TWO PATHS?\n     - Path A preserves all data quickly (backup + immediate availability)\n     - Path B does expensive AI processing in separate workflow\n     - Separation prevents workflow complexity and enables independent testing\n\n     2. FALSE BRANCH (no jobs found):\n     → Log no results → Insert bad row → Loop back\n     - Creates error record in database\n     - Allows company loop to continue\n\n     Important behavior:\n     If 8 jobs found, this IF node evaluates 8 TIMES (once per job).\n     Each job flows through Add Context, then splits to both paths.\n\n     Next nodes:\n     - TRUE → Add Context (then splits to Save + Call sub-workflow)\n     - FALSE → Log no results\n"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE 8: SAVE RAW JOB DATA\n// =====================================================================\n\n// STEP 1: Get all jobs for current company (with context already added)\nconst currentJobs = $input.all().map(item => item.json);\n\n// STEP 2: Get current company context from loop\n// We need this to populate company-related fields in the database\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// STEP 3: Transform each job into database row format\n// Creates one database row per job with extracted searchable fields\nreturn currentJobs.map(job => ({\n  json: {\n    // === COMPANY CONTEXT ===\n    // Links this job back to the company that posted it\n    company_id: job._context_company_id,\n    company_name: job._context_company_name,\n    domain_guess: currentCompany.domain,\n    \n    // === JOB IDENTIFIERS ===\n    job_id: job.id || '',  // Apify's unique job ID\n    job_title: job.title || '',  // \"Senior Product Manager\"\n    organization: job.organization || '',  // \"Anthropic\"\n    organization_url: job.organization_url || '',  // Link to company page\n    \n    // === DATES ===\n    date_posted: job.date_posted || null,  // When job was posted\n    date_created: job.date_created || null,  // When Apify first found it\n    \n    // === LOCATION ===\n    // Extract first location from locations_raw array\n    // The ?. is \"optional chaining\" - prevents errors if field doesn't exist\n    location_locality: job.locations_raw?.[0]?.address?.addressLocality || '',  // City\n    location_region: job.locations_raw?.[0]?.address?.addressRegion || '',  // State\n    location_country: job.locations_raw?.[0]?.address?.addressCountry || '',  // Country\n    remote_derived: job.remote_derived || false,  // Is it remote?\n    \n    // === SALARY (AI-EXTRACTED BY APIFY) ===\n    salary_currency: job.ai_salary_currency || '',  // \"USD\"\n    salary_min: job.ai_salary_minvalue || null,  // 200000\n    salary_max: job.ai_salary_maxvalue || null,  // 250000\n    salary_unit: job.ai_salary_unittext || '',  // \"YEAR\"\n    \n    // === JOB DETAILS ===\n    // Convert array to comma-separated string if needed\n    employment_type: Array.isArray(job.employment_type)\n      ? job.employment_type.join(', ')  // [\"FULL_TIME\"] → \"FULL_TIME\"\n      : '',\n    experience_level: job.ai_experience_level || '',  // \"5-10\" years\n    work_arrangement: job.ai_work_arrangement || '',  // \"Hybrid\", \"Remote\"\n    \n    // === SOURCE/ATS PLATFORM ===\n    source_ats: job.source || '',  // \"greenhouse\", \"lever\", etc.\n    source_domain: job.source_domain || '',  // \"jobs.lever.co\"\n    domain_derived: job.domain_derived || '',  // Company's actual domain\n    \n    // === LINKS ===\n    job_url: job.url || '',  // Direct link to apply\n    \n    // === FULL JSON BACKUP ===\n    // Store the COMPLETE job object as JSON string\n    // This preserves everything in case we need it later\n    // Includes _context_ fields added by Add Context node\n    job_data_full: JSON.stringify(job)\n  }\n}));\n\n// WHAT GETS OUTPUT:\n// One database row per job with extracted fields\n// Example for 8 jobs: 8 separate database rows\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1872,
        -96
      ],
      "id": "c1e258ac-bf92-4336-916c-f703a333c56d",
      "name": "Save Apify data",
      "notes": " ========================================================================\n     NODE 9: SAVE RAW JOB DATA\n     Extracts key fields from Apify jobs for database storage     =========================================================================\n     PURPOSE:\n     This is the \"fast path\" - immediate backup of all job data.\n     Runs in PARALLEL with the AI evaluation sub-workflow.\n     Completes in ~100ms while AI evaluation takes 2-5 seconds per job.\n\n     STEP 1: Get all jobs for current company (with context already added)\n     const currentJobs = $input.all().map(item =>\n     item.json);\n\n     STEP 2: Get current company context from loop\n     We need this to populate company-related fields in the database\n\n     STEP 3: Transform each job into database row format\n     Creates one database row per job with extracted searchable fields\n     return currentJobs.map(job => ({\n       json: {\n         // === COMPANY CONTEXT ===\n     // Links this job back to the company that posted it\n         company_id: currentCompany.company_id,\n         company_name: currentCompany.company_name,\n         domain_guess: currentCompany.domain,\n\n         // === JOB IDENTIFIERS ===\n     job_id: job.id || '',  // Apify's unique job ID\n     job_title: job.title || '',  // \"Senior Product Manager\"\n     organization: job.organization || '',  // \"Anthropic\"\n     organization_url: job.organization_url || '',  //  Link to company page\n\n         // === DATES ===\n     date_posted: job.date_posted || null,  // When job was posted\n     date_created: job.date_created || null,  // When Apify first found it\n\n         // === LOCATION ===\n     // Extract first location from locations_raw array\n     // The ?. is \"optional chaining\" - prevents errors if field doesn't exist\n     location_locality:\n     job.locations_raw?.[0]?.address?.addressLocality || '',  // City\n     location_region:\n     job.locations_raw?.[0]?.address?.addressRegion || '',  // State\n     location_country:\n     job.locations_raw?.[0]?.address?.addressCountry || '',  // Country\n     remote_derived: job.remote_derived || false,  // Is it remote?\n\n         // === SALARY (AI-EXTRACTED BY APIFY) ===\n     salary_currency: job.ai_salary_currency || '',  // \"USD\"\n     salary_min: job.ai_salary_minvalue || null,  // 200000\n     salary_max: job.ai_salary_maxvalue || null,  // 250000\n     salary_unit: job.ai_salary_unittext || '',  //  \"YEAR\"\n\n         // === JOB DETAILS ===\n     // Convert array to comma-separated string if needed\n     employment_type:\n     Array.isArray(job.employment_type)\n     ? job.employment_type.join(', ')  // [\"FULL_TIME\"]\n     → \"FULL_TIME\"\n           : '',\n     experience_level: job.ai_experience_level || '', // \"5-10\" years\n     work_arrangement: job.ai_work_arrangement || '', // \"Hybrid\", \"Remote\"\n\n         // === SOURCE/ATS PLATFORM ===\n     source_ats: job.source || '',  // \"greenhouse\", \"lever\", etc.\n     source_domain: job.source_domain || '',  // \"jobs.lever.co\"\n     domain_derived: job.domain_derived || '',  // Company's actual domain\n\n         // === LINKS ===\n     job_url: job.url || '',  // Direct link to apply\n\n         // === FULL JSON BACKUP ===\n     // Store the COMPLETE job object as JSON string\n     // This preserves EVERYTHING including _context_ fields\n     // IMPORTANT: This includes profile data from Add Context node!\n     // The _context_resume_text and other profile fields are preserved here\n         job_data_full: JSON.stringify(job)\n       }\n     }));\n\n     WHAT GETS OUTPUT:\n     One database row per job with extracted fields\n     Example for 8 jobs: 8 separate database rows\n\n     WHY WE DO THIS:\n     1. SEARCHABLE FIELDS: Can query by salary, location, title without parsing JSON\n     2. CSV-FRIENDLY: All fields are simple strings/numbers\n     3. COMPLETE BACKUP: job_data_full preserves everything including _context_ fields\n     4. PERFORMANCE: Don't need to parse JSON for common queries\n     5. IMMEDIATE AVAILABILITY: Jobs visible in database while AI evaluation runs\n     6. PROFILE PRESERVATION: job_data_full includes _context_resume_text and other profile fields\n\n     ARCHITECTURAL NOTE:\n     This is the \"fast path\" that runs PARALLEL to AI evaluation.\n     In v5, this path looped back. In the new architecture:\n     - This path: Saves and ENDS (no loop back)\n     - Other path: Calls sub-workflow which loops back when done\n     This prevents race conditions and simplifies flow control.\n\n     Next node: Insert row (saves to database, then ends)\n"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "iiSsg3nWcVECYUyk",
          "mode": "list",
          "cachedResultName": "Jobs",
          "cachedResultUrl": "/projects/NRVzB7ls7lOjZ60v/datatables/iiSsg3nWcVECYUyk"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "company_id": "={{ $json.company_id }}",
            "company_name": "={{ $json.company_name }}",
            "domain_derived": "={{ $json.domain_derived }}",
            "job_id": "={{ $json.job_id }}",
            "job_title": "={{ $json.job_title }}",
            "date_posted": "={{ $json.date_posted }}",
            "locality": "={{ $json.location_locality }}",
            "region": "={{ $json.location_region }}",
            "remote_derived": "={{ $json.remote_derived }}",
            "salary_min": "={{ $json.salary_min }}",
            "salary_max": "={{ $json.salary_max }}",
            "job_url": "={{ $json.job_url }}",
            "job_data": "={{ $json.job_data_full }}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "company_id",
              "displayName": "company_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "company_name",
              "displayName": "company_name",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domain_derived",
              "displayName": "domain_derived",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_id",
              "displayName": "job_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_title",
              "displayName": "job_title",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "date_posted",
              "displayName": "date_posted",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "locality",
              "displayName": "locality",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "region",
              "displayName": "region",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "remote_derived",
              "displayName": "remote_derived",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "salary_min",
              "displayName": "salary_min",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "salary_max",
              "displayName": "salary_max",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_url",
              "displayName": "job_url",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_data",
              "displayName": "job_data",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1.1,
      "position": [
        2080,
        -96
      ],
      "id": "30a0a385-151e-4378-9019-947a174ee1d8",
      "name": "Insert row",
      "notes": " ═════════════════════════\n     NODE 10: INSERT RAW JOBS TO DATABASE\n(FAST PATH)    ═════════════════════════\n     Inserts raw job data into the 'jobs' database table.\n\n     What this node does:\n     - Takes formatted job data from Save Apify node\n     - Inserts ONE ROW PER JOB into database\n     - Completes quickly (~100ms per job) while AI\n     evaluation runs in parallel\n\n     Why save raw jobs to database?\n     1. IMMEDIATE BACKUP: Preserves all job data before any processing\n     2. SEARCHABLE: Can query by salary, location,\n     company without AI evaluation\n     3. HISTORICAL RECORD: Track jobs over time, even if AI evaluation fails\n     4. DEBUGGING: Compare raw data vs AI evaluation\n     results\n     5. FAIL-SAFE: If sub-workflow fails, we still have\n     the raw job data\n     6. PROFILE BACKUP: job_data_full includes\n     _context_ profile fields\n\n     Example: If Anthropic has 8 jobs, this node\n     executes 8 times,\n     creating 8 separate database rows.\n\n     Field mapping explanation:\n     ={{ $json.company_id }} means \"read company_id\n     field from input data\"\n\n     Database columns stored:\n     - company_id, company_name, domain_derived:\n     Company identification\n     - job_id, job_title, job_url: Job identifiers and links\n     - locality, region, remote_derived: Location\n     information\n     - salary_min, salary_max: Compensation range\n     - job_data: Complete JSON backup (includes\n     _context_ fields with profile data)\n\n     Parallel execution:\n     This node runs PARALLEL to \"Call 'Loop Jobs'\"\n     sub-workflow.\n     - This path: Fast storage, completes in milliseconds\n     - Other path: AI evaluation, takes 2-5 sec per job\n     - Both paths must complete before company loop\n     advances\n\n     Note about profile data:\n     The job_data column contains the full JSON\n     including _context_resume_text and other profile\n     fields. This means the raw job backup preserves\n     the candidate profile that was used for evaluation\n     context.\n\n     Next: [No connection - path ends here]\n     Workflow waits for parallel \"Call 'Loop Jobs'\"\n     path to complete"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE 12: LOG ERROR\n// =====================================================================\n\n// What this node does:\n// When Apify API call completely fails (timeout, auth error,\n// network issue), we log the error so we can debug and retry later.\n\n// Get current company from the company loop\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// Get error details from Apify node's error output\n// The Run Apify node passes error information through $json.error\nconst errorInfo = $json.error || { message: 'Unknown error' };\n\n// Create error record for database\nreturn [{\n  json: {\n    company_id: currentCompany.company_id,\n    company_name: currentCompany.company_name,\n    domain_guess: currentCompany.domain,\n    \n    status: 'api_error',  // Indicates API call failed completely\n    jobs_found: 0,\n    \n    // No domain verification data\n    domain_verified: null,\n    domains_seen: '',\n    ats_platforms_seen: '',\n    ats_type: '',\n    \n    // Include error message for debugging\n    note: `Apify API Error: ${errorInfo.message || JSON.stringify(errorInfo)}`,\n    \n    // Timestamp\n    verified_at: new Date().toISOString()\n  }\n}];\n\n// COMMON ERROR TYPES:\n// - Timeout: API took too long to respond (>30 seconds)\n// - Authentication: API key invalid, expired, or missing\n// - Rate limit: Too many requests in short period\n// - Network: Connection issues, DNS problems, firewall\n// - 500 error: Apify server problem\n// - 400 error: Invalid request parameters\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1456,
        288
      ],
      "id": "c472b857-680e-406d-88f3-6aa8aacac1eb",
      "name": "Log Error",
      "notes": " ========================================================================\n     NODE 13: LOG ERROR\n     Formats Apify API errors for database insertion  =========================================================================\n     What this node does:\n     When Apify API call completely fails (timeout,\n     auth error, network issue),\n     we log the error so we can debug and retry later.\n\n     Get current company from the company loop\n     const currentCompany = $('Loop Over\n     Companies').item.json;\n\n     Get error details from Apify node's error output\n     The Run Apify node passes error information through $json.error\n     const errorInfo = $json.error || { message: 'Unknown error' };\n\n     Create error record for database\n     return [{\n       json: {\n         company_id: currentCompany.company_id,\n         company_name: currentCompany.company_name,\n         domain_guess: currentCompany.domain,\n\n     status: 'api_error',  // Indicates API call failed completely\n         jobs_found: 0,\n\n         // Include error message for debugging\n     note: `Apify API Error: ${errorInfo.message ||\n     JSON.stringify(errorInfo)}`,\n\n         // Timestamp\n         verified_at: new Date().toISOString()\n       }\n     }];\n\n     COMMON ERROR TYPES:\n     - Timeout: API took too long to respond (>30 seconds)\n     - Authentication: API key invalid, expired, or missing\n     - Rate limit: Too many requests in short period\n     - Network: Connection issues, DNS problems, firewall\n     - 500 error: Apify server problem\n     - 400 error: Invalid request parameters (check Build Request node)\n\n     WHAT TO DO WHEN ERRORS OCCUR:\n     1. Check error message in database after workflow completes\n     2. Verify Apify credentials in n8n settings\n     3. Check Apify service status (status.apify.com)\n     4. Review request parameters in Build Request node\n     5. Verify target_criteria JSON is valid in profile\n     6. Re-run workflow for failed companies\n     7. Contact Apify support if persistent issues\n\n     DEBUGGING TIPS:\n     - View full error in n8n execution log\n     - Check Apify dashboard for account status\n     - Verify domain format in companies table\n     - Test with known-good company (e.g. \"anthropic.com\")\n     - Check if profile.target_criteria is valid JSON\n\n     PROFILE CONTEXT:\n     Although we have access to profile fields, we\n     don't include them in the\n     error log. API errors are typically infrastructure\n     issues, not profile issues.\n\n     Next node: Insert bad row (saves to errors table)"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE 11: LOG NO RESULTS\n// =====================================================================\n\n// What this node does:\n// When Apify successfully calls a company but finds 0 matching jobs,\n// we still want to track this (so we know the company was checked).\n\n// Get current company from the company loop\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// Create error record for database\nreturn [{\n  json: {\n    company_id: currentCompany.company_id,\n    company_name: currentCompany.company_name,\n    \n    status: 'no_results',  // Indicates API worked but 0 jobs found\n    jobs_found: 0,\n        \n    // Explanation\n    note: 'API succeeded but returned 0 jobs - company not found in Apify or no matching PM roles',\n    \n    // Timestamp\n    verified_at: new Date().toISOString()\n  }\n}];\n\n// Possible reasons for 0 jobs:\n// 1. Company not currently hiring PMs\n// 2. Company name mismatch (\"OpenAI\" vs \"Open AI\" vs \"OpenAI Inc\")\n// 3. Company not in Apify database\n// 4. Our filters too restrictive (title, remote, timeRange)\n// 5. All PM jobs filled/closed recently\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1648,
        96
      ],
      "id": "82c7617d-dd2d-4052-b444-3868e4468e1d",
      "name": "Log no results",
      "notes": " ========================================================================\n     NODE 12: LOG NO RESULTS\n     Handles successful API calls that returned 0 jobs    =========================================================================\n     What this node does:\n     When Apify successfully calls a company but finds 0 matching jobs,\n     we still want to track this (so we know the company was checked).\n\n     Get current company from the company loop\n     Note: We can access both company and profile\n     fields from the loop context\n     because they were merged in the Merge Data node\n     const currentCompany = $('Loop Over\n     Companies').item.json;\n\n     Create error record for database\n     return [{\n       json: {\n         company_id: currentCompany.company_id,\n         company_name: currentCompany.company_name,\n         domain_guess: currentCompany.domain,\n\n     status: 'no_results',  // Indicates API worked but 0 jobs found\n         jobs_found: 0,\n\n         // Explanation note: 'API succeeded but returned 0 jobs - company not found in Apify or no matching PM roles',\n\n         // Timestamp\n         verified_at: new Date().toISOString()\n       }\n     }];\n\n     WHY THIS MATTERS:\n     - Distinguishes between \"no jobs\" (tracked here) and \"API error\" (Log Error)\n     - Helps identify companies to remove or update in companies table\n     - Tracks coverage (which companies we're monitoring)\n     - Provides audit trail of workflow execution\n\n     Possible reasons for 0 jobs:\n     1. Company not currently hiring PMs\n     2. Company name mismatch (\"OpenAI\" vs \"Open AI\" vs \"OpenAI Inc\")\n     3. Company not in Apify database\n     4. Our filters too restrictive (title, remote, timeRange)\n     5. All PM jobs filled/closed recently\n     6. Target criteria from profile too specific (titleSearch, etc.)\n\n     PROFILE CONTEXT:\n     Although we have access to profile fields via\n     currentCompany.resume_text,\n     we don't include them in the error log. The error\n     tracking is focused on\n     company-level issues, not profile-specific issues.\n\n     Next node: Insert bad row (saves to errors table)"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "e4mOnHpKZpmpIc6l",
          "mode": "list",
          "cachedResultName": "Errors",
          "cachedResultUrl": "/projects/NRVzB7ls7lOjZ60v/datatables/e4mOnHpKZpmpIc6l"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "company_id": "={{ $json.company_id }}",
            "company_name": "={{ $json.company_name }}",
            "status": "={{ $json.status }}",
            "jobs_found": "={{ $json.jobs_found }}",
            "note": "={{ $json.note }}",
            "verified_at": "={{ $json.verified_at }}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "company_id",
              "displayName": "company_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "company_name",
              "displayName": "company_name",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domain_guess",
              "displayName": "domain_guess",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "domain_verified",
              "displayName": "domain_verified",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "ats_type",
              "displayName": "ats_type",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "jobs_found",
              "displayName": "jobs_found",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "ats_platforms_seen",
              "displayName": "ats_platforms_seen",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "domains_seen",
              "displayName": "domains_seen",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": true
            },
            {
              "id": "status",
              "displayName": "status",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "note",
              "displayName": "note",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "verified_at",
              "displayName": "verified_at",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1.1,
      "position": [
        1872,
        288
      ],
      "id": "fb445e54-458c-4b9b-86ad-14b130318e1f",
      "name": "Insert bad row",
      "notes": " ═════════════════════════\n     NODE 14: SAVE ERRORS TO DATABASE     ═════════════════════════\n     Inserts error records (no results + API failures)\n     to 'errors 1' table.\n\n     What this node does:\n     - Saves error/no-result records to database for\n     tracking and debugging\n     - Receives from TWO sources:\n     1. Log no results (status: 'no_results') - API\n     succeeded but 0 jobs\n     2. Log Error (status: 'api_error') - API call\n     failed completely\n\n     Why track errors in database?\n     1. DEBUGGING: Which companies failed and why\n     2. MONITORING: Track API reliability over time\n     3. MAINTENANCE: Identify companies to update or\n     remove from monitoring list\n     4. COVERAGE: Verify all companies were checked\n     during workflow run\n     5. AUDIT TRAIL: Historical record of workflow\n     execution issues\n\n     Status types stored:\n     - 'no_results': API succeeded, but 0 jobs found\n     for company\n     - Possible reasons: Not hiring, name mismatch, not\n     in Apify\n     - Could also be due to target_criteria from\n     profile being too restrictive\n     - 'api_error': API call completely failed\n     - Possible reasons: Timeout, auth error, network\n     issue, rate limit\n     - Could also be due to invalid target_criteria\n     JSON in profile\n\n     All error types go to the SAME table for\n     centralized tracking.\n     This allows queries like: \"Show me all errors from\n     last week\"\n\n     Production monitoring:\n     - Schedule daily query to check error count\n     - Alert if error rate exceeds threshold (e.g., >10%)\n     - Review no_results companies monthly\n     - Update company names or remove inactive\n     companies\n     - Check if target_criteria needs adjustment for\n     profile\n\n     Profile context:\n     Although errors are tracked at company level,\n     remember that the workflow\n     is now profile-aware. If many companies show\n     no_results, consider checking\n     if profile.target_criteria is too restrictive\n     (e.g., titleSearch too specific).\n\n     Next node: Loop Over Companies (loops back for\n     next company)\n"
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "value": "opBrfNaudhVTNj6p",
          "mode": "list",
          "cachedResultUrl": "/workflow/opBrfNaudhVTNj6p",
          "cachedResultName": "Loop Jobs v4.1"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {},
          "matchingColumns": [],
          "schema": [],
          "attemptToConvertTypes": false,
          "convertFieldsToString": true
        },
        "options": {}
      },
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.3,
      "position": [
        2256,
        320
      ],
      "id": "ea8d513b-9280-41d2-9746-c784d0c27f84",
      "name": "Call 'Loop Jobs v4.1'",
      "notes": " ═════════════════════════\n     NODE 11: CALL SUB-WORKFLOW\n(EXECUTE WORKFLOW)    ═════════════════════════\n     Calls the \"Loop Jobs\" workflow to process\n     each job with AI evaluation.\n\n     What this node does:\n     1. Receives jobs with context from \"Add Context\"\n     node\n     2. Calls \"Loop Jobs v4.1\" workflow\n     3. Passes ALL job data (including _context_\n     fields) to sub-workflow\n     4. Waits for sub-workflow to complete ALL jobs for\n     current company\n     5. Returns to company loop when done\n\n     Sub-workflow (Loop Jobs v4.1) handles:\n     - Individual job iteration (loops through jobs one\n     at a time)\n     - AI evaluation with Claude Sonnet 4.5 using\n     _context_resume_text\n     - Parsing AI response\n     - Merging AI evaluation with raw job data\n     - Formatting HTML job cards\n     - Saving to email queue table\n\n     Why separate workflows?\n     1. SEPARATION OF CONCERNS:\n     - Company workflow: Orchestration and\n     company-level operations\n     - Job workflow: Individual job processing and AI\n     evaluation\n\n     2. INDEPENDENT TESTING:\n     - Test job evaluation without running company loop\n     - Can test company discovery without AI evaluation\n\n     3. EASIER DEBUGGING:\n     - Isolate issues to specific workflow\n     - View execution logs separately\n\n     4. REUSABILITY:\n     - Job workflow could be triggered from other\n     sources\n     - E.g., manual job evaluation, webhook triggers\n\n     5. CLEANER ARCHITECTURE:\n     - No nested loops in single workflow\n     - Clear workflow boundaries and responsibilities\n\n     Data flow across workflow boundary:\n     Parent (this workflow) passes to sub-workflow:\n     - All Apify job fields (id, title, salary,\n     location, description, etc.)\n     - _context_workflow_run_id (parent execution ID\n     for grouping)\n     - _context_company_id (company identifier)\n     - _context_company_name (company display name)\n     - _context_domain (company domain)\n     - _context_profile_id (candidate profile\n     identifier)\n     - _context_resume_text (full resume for AI\n     evaluation)\n     - _context_target_criteria (job search criteria\n     JSON)\n\n     Sub-workflow returns:\n     Nothing explicit - it saves directly to email\n     queue table.\n     The parent workflow continues when sub-workflow execution completes.\n\n     Timing:\n     - If 8 jobs found for Anthropic\n     - This node is called with all 8 jobs\n     - Sub-workflow processes them one-by-one (internal loop)\n     - Takes 2-5 seconds per job = 16-40 seconds total\n     - Returns control after all 8 jobs processed\n\n     Parallel execution:\n     This node runs in PARALLEL with \"Save Apify data\"\n     path:\n     - Save path: Completes in milliseconds\n     - This path: Completes in seconds (AI evaluation)\n     - Company loop waits for BOTH to complete\n\n     - After sub-workflow completes all jobs for\n     current company\n     - Loops back to \"Loop Over Companies\" node\n     - Company loop advances to next company\n"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE: ADD CONTEXT TO JOBS\n// =====================================================================\n\n// STEP 1: Get all jobs returned from Apify\nconst jobs = $input.all().map(item => item.json);\n\n// STEP 2: Get the current iteration's company+profile data from Merge Data node\n// This has the original merged data before Build Request transformed it\nconst currentIteration = $('Merge Data').item.json;\n\n// STEP 3: Attach context to each job\nreturn jobs.map(job => ({\n  json: {\n    // Preserve all original Apify job fields\n    ...job,\n    \n    // Add parent workflow context for tracking across workflow boundary\n    _context_workflow_run_id: $execution.id,\n    \n    // Company context\n    _context_company_id: currentIteration.company_id,\n    _context_company_name: currentIteration.company_name,\n    _context_domain: currentIteration.domain,\n    \n    // Profile context (for AI evaluation in Loop Jobs)\n    _context_profile_id: currentIteration.profile_id,\n    _context_resume_text: currentIteration.resume_text,\n    _context_target_criteria: currentIteration.target_criteria\n  }\n}));\n\n// OUTPUT: Jobs with both company AND profile context fields\n// These flow through to Loop Jobs where AI can use resume_text for evaluation"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1456,
        -96
      ],
      "id": "a1ede68b-1a7b-4e46-bf23-f1f2ffbe0aa6",
      "name": "Add Context - Pass",
      "notes": " ========================================================================\n     NODE 8a: ADD CONTEXT TO JOBS (PASS)\n     Purpose: Enriches Apify jobs with parent workflow context AND profile data     =========================================================================\n     This node adds PROFILE CONTEXT in addition to company context.\n     The profile fields enable the Loop Jobs sub-workflow to perform AI evaluation\n     without needing to load profile data separately.\n\n     STEP 1: Get all jobs returned from Apify for current company\n     const jobs = $input.all().map(item => item.json);\n\n     STEP 2: Get the current iteration's company+profile data from Merge Data node\n     This contains the merged data BEFORE it was transformed into Apify request format\n     We need the original merged data because it has profile fields that were\n     stripped out by Build Request (Apify doesn't need them)\n     const currentIteration = $('Merge Data').item.json;\n\n     STEP 3: Attach context to each job\n     This creates a comprehensive context bundle that includes:\n     - All original Apify job fields\n     - Company identification\n     - Profile data for AI evaluation\n     - Workflow tracking IDs\n     return jobs.map(job => ({\n       json: {\n     // === PRESERVE ALL ORIGINAL APIFY JOB FIELDS ===\n     // The spread operator (...job) copies ALL fields from the job\n     // This includes: id, title, description, salary, location, etc.\n         ...job,\n\n         // === PARENT WORKFLOW TRACKING ===\n     // This workflow's execution ID - used to group all jobs from this run\n         _context_workflow_run_id: $execution.id,\n\n         // === COMPANY CONTEXT ===\n         // Identifies which company posted this job\n     _context_company_id: currentIteration.company_id,\n     _context_company_name:\n     currentIteration.company_name,\n         _context_domain: currentIteration.domain,\n\n         // === PROFILE CONTEXT (NEW IN v2.1) ===\n     // Provides candidate profile data for AI evaluation in Loop Jobs\n     _context_profile_id: currentIteration.profile_id,\n     _context_resume_text:\n     currentIteration.resume_text,\n     _context_target_criteria:\n     currentIteration.target_criteria\n       }\n     }));\n\n     OUTPUT: Jobs with comprehensive context\n     Example structure for one job:\n     {\n     // Original Apify fields:\n     id: \"job_12345\",\n     title: \"Senior Product Manager\",\n     description_text: \"We are looking for...\",\n     ai_salary_minvalue: 200000,\n     ai_salary_maxvalue: 250000,\n     ... (all other Apify fields),\n\n     // Context fields:\n     _context_workflow_run_id: 67890,\n     _context_company_id: \"anthropic\",\n     _context_company_name: \"Anthropic\",\n     _context_domain: \"anthropic.com\",\n     _context_profile_id: \"[id]\",\n     _context_resume_text: \"[resume]\",\n     _context_target_criteria: \"{\\\"titleSearch\\\":\n     [\\\"product manager\\\"], ...}\"\n     }\n\n     WHY ADD PROFILE CONTEXT HERE?\n     1. CROSS-WORKFLOW COMMUNICATION: Sub-workflow needs profile for AI evaluation\n     2. SINGLE BUNDLE: All data needed for job evaluation in one package\n     3. NO ADDITIONAL LOOKUPS: Loop Jobs doesn't need to query profile database\n     4. CONSISTENCY: Each job carries identical profile context (no race conditions)\n\n     DATA FLOW:\n     These enriched jobs flow through TWO parallel paths:\n     Path A: Save Apify data → Database (preserves\n     _context_ fields in job_data_full)\n     Path B: Call Loop Jobs → AI evaluation (uses\n     _context_resume_text)\n\n     Next nodes (PARALLEL):\n     - Save Apify data (fast path - backup to database)\n     - Call 'Loop Jobs' sub-workflow (slow path - AI\n     evaluation)\n"
    },
    {
      "parameters": {
        "jsCode": "// =====================================================================\n// NODE: ADD CONTEXT TO JOBS\n// =====================================================================\n\n// STEP 1: Get all jobs returned from Apify\nconst jobs = $input.all().map(item => item.json);\n\n// STEP 2: Get the current iteration's company+profile data from Merge Data node\n// This has the original merged data before Build Request transformed it\nconst currentIteration = $('Merge Data').item.json;\n\n// STEP 3: Attach context to each job\nreturn jobs.map(job => ({\n  json: {\n    // Preserve all original Apify job fields\n    ...job,\n    \n    // Add parent workflow context for tracking across workflow boundary\n    _context_workflow_run_id: $execution.id,\n    \n    // Company context\n    _context_company_id: currentIteration.company_id,\n    _context_company_name: currentIteration.company_name,\n    _context_domain: currentIteration.domain,\n    \n    // Profile context (for AI evaluation in Loop Jobs)\n    _context_profile_id: currentIteration.profile_id,\n    _context_resume_text: currentIteration.resume_text,\n    _context_target_criteria: currentIteration.target_criteria\n  }\n}));\n\n// OUTPUT: Jobs with both company AND profile context fields\n// These flow through to Loop Jobs where AI can use resume_text for evaluation"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1456,
        96
      ],
      "id": "982746cb-afa1-40aa-a11d-fbc24849fca3",
      "name": "Add Context - Fail",
      "notes": " ========================================================================\n     NODE 8b: ADD CONTEXT TO JOBS (FAIL)\n     Purpose: Enriches Apify jobs with parent workflow context AND profile data     =========================================================================\n     This node adds PROFILE CONTEXT in addition to company context.\n\n     Next nodes:\n     - Write no data\n"
    },
    {
      "parameters": {
        "operation": "Run actor and get dataset",
        "actorId": {
          "__rl": true,
          "value": "s3dtSTZSZWFtAVLn5",
          "mode": "list",
          "cachedResultName": "Career Site Job Listing API (fantastic-jobs/career-site-job-listing-api)",
          "cachedResultUrl": "https://console.apify.com/actors/s3dtSTZSZWFtAVLn5/input"
        },
        "customBody": "={{ $json }}",
        "authentication": "apifyOAuth2Api"
      },
      "type": "@apify/n8n-nodes-apify.apify",
      "typeVersion": 1,
      "position": [
        1040,
        96
      ],
      "id": "fe905d75-645c-4add-890f-fb788612c52f",
      "name": "Run Apify",
      "alwaysOutputData": true,
      "credentials": {
        "apifyOAuth2Api": {
          "id": "hwrvawIhCz4iflM8",
          "name": "Apify account"
        }
      },
      "onError": "continueErrorOutput",
      "notes": " ═════════════════════════\n     NODE 6: CALL APIFY JOB SEARCH API     ═════════════════════════\n     Calls Apify's Career Site Job Listing API to\n     search for jobs.\n     - Third-party job search service that crawls\n     company career pages\n     - Aggregates jobs from ATS platforms (Greenhouse, Lever, Workday, etc.)\n     - Includes AI-powered extraction of salary,\n     location, experience level\n\n     What this node does:\n     1. Receives ONE company request (from loop)\n     2. Calls Apify API with search parameters from\n     Build Request node\n     3. Returns 0-10 job listings for that company\n\n     Critical settings explained:\n     - alwaysOutputData: true\n     → Ensures node outputs data even if 0 jobs found\n     → Without this, 0 jobs would skip downstream nodes\n\n     - onError: continueErrorOutput\n     → If API fails, route to error branch instead of\n     crashing workflow\n     → Enables fault tolerance\n\n     → Send ALL fields from previous node (Build\n     Request) to API\n     → Includes Apify search parameters (domainFilter,\n     titleSearch, etc.)\n\n     Two outputs (important!):\n     1. SUCCESS BRANCH (top pin):\n     - API call succeeded (even if 0 jobs returned)\n     - Routes to: IF node (check if jobs exist)\n\n     2. ERROR BRANCH (bottom pin):\n     - API call failed (network error, timeout,\n     authentication failure)\n     - Routes to: Log Error node\n\n     Example scenarios:\n     ┌─ Anthropic: Returns 8 jobs → Success branch with 8 items\n     ├─ Fictional company: Returns 0 jobs → Success\n     branch with 0 items\n     └─ API timeout: → Error branch with error object\n\n     Automatic iteration:\n     If Apify returns 8 jobs, n8n AUTOMATICALLY\n     processes each job\n     individually through downstream nodes. No explicit\n     job loop needed here.\n\n     Each job item includes:\n     - Basic info: id, title, organization, url\n     - Location: cities_derived, regions_derived,\n     remote_derived\n     - Salary: ai_salary_minvalue, ai_salary_maxvalue,\n     ai_salary_currency\n     - Details: description_text, ai_experience_level,\n     ai_work_arrangement\n\n     Next nodes:\n     - Success → IF node (check if jobs exist)\n     - Error → Log Error node\n"
    }
  ],
  "pinData": {},
  "connections": {
    "Start": {
      "main": [
        [
          {
            "node": "Load Companies",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Load Companies": {
      "main": [
        [
          {
            "node": "Merge Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Data": {
      "main": [
        [
          {
            "node": "Build Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Request": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Loop Over Companies": {
      "main": [
        [
          {
            "node": "Workflow Summary",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Run Apify",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If": {
      "main": [
        [
          {
            "node": "Add Context - Pass",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Add Context - Fail",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save Apify data": {
      "main": [
        [
          {
            "node": "Insert row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Error": {
      "main": [
        [
          {
            "node": "Insert bad row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log no results": {
      "main": [
        [
          {
            "node": "Insert bad row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert bad row": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call 'Loop Jobs v4.1'": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Add Context - Pass": {
      "main": [
        [
          {
            "node": "Save Apify data",
            "type": "main",
            "index": 0
          },
          {
            "node": "Call 'Loop Jobs v4.1'",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Add Context - Fail": {
      "main": [
        [
          {
            "node": "Log no results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run Apify": {
      "main": [
        [
          {
            "node": "If",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Log Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1",
    "availableInMCP": false
  },
  "versionId": "d48f5ac1-055e-4b46-9c1b-a1c6f4fd3ca1",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "b50b11e8b95a97f785a59928b1142bfe5660398bf6359dbc9cca38ca2424c956"
  },
  "id": "aDwfiKMpyD7fz58g",
  "tags": []
}