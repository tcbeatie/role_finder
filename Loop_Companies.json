{
  "name": "Loop Companies v2.1 - FULLY DOCUMENTED",
  "nodes": [
    {
      "parameters": {},
      "type": "n8n-nodes-base.manualTrigger",
      "typeVersion": 1,
      "position": [
        -1072,
        160
      ],
      "id": "c3267bd8-1991-43e2-a06e-6c1348f86450",
      "name": "Start",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 1: MANUAL TRIGGER + PROFILE LOADER\n═══════════════════════════════════════════════════════════════════════════\nWorkflow Entry Point - This is where the Company workflow starts!\n\nWhat it does:\n- Starts the workflow when you click the 'Execute workflow' button in n8n UI\n- No configuration or parameters needed in the node itself\n- Provides PROFILE DATA through trigger execution context\n\nARCHITECTURAL CHANGE (v2.1):\nIn previous versions, candidate profile data was hardcoded in the workflow.\nNow, profile data is loaded externally and passed via the trigger.\n\nProfile data structure expected:\n{\n  profile_id: string,           // \"ted_beatie_pm\"\n  profile_name: string,          // \"Ted Beatie - Technical PM\"\n  resume_text: string,           // Full resume/experience text\n  target_criteria: string,       // JSON string with job search criteria\n  notes: string                  // Additional profile notes\n}\n\nWhere does profile data come from?\n- Option A: Passed from parent workflow (if this becomes a sub-workflow)\n- Option B: Loaded from candidate_profile database table\n- Option C: Provided via webhook payload\n- Current: Set via manual execution context for testing\n\nHow profile flows through workflow:\n1. Start trigger provides profile data\n2. Merge Data combines profile with each company\n3. Build Request extracts criteria from profile.target_criteria\n4. Add Context passes profile fields to Loop Jobs sub-workflow\n5. Loop Jobs uses resume_text for AI evaluation\n\nBenefits of externalization:\n- No hardcoded personal data in workflow JSON\n- Easy profile updates without editing workflow\n- Multi-user support (different profiles for different users)\n- Version control friendly (workflow doesn't change with profile updates)\n- Secure (profile can be stored encrypted in database)\n\nNext nodes:\n- Load Companies (loads target companies in parallel)\n- Merge Data (receives profile from this node)\n\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "operation": "get",
        "dataTableId": {
          "__rl": true,
          "value": "2or5DyUM3HlNIgTd",
          "mode": "list",
          "cachedResultName": "companies",
          "cachedResultUrl": "/projects/MIniGOtG65wFTuKC/datatables/2or5DyUM3HlNIgTd"
        },
        "returnAll": true
      },
      "id": "988c647d-c3f6-42d1-bba7-71f8ca5a50d7",
      "name": "Load Companies",
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        -864,
        0
      ],
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 2: LOAD COMPANIES TO MONITOR\n═══════════════════════════════════════════════════════════════════════════\nLoads target companies from n8n data table.\n\nWhat it does:\n- Connects to n8n data table called 'companies'\n- Loads ALL rows (returnAll: true means no limit)\n- Each row represents one company to monitor for jobs\n\nExpected table columns:\n- company_id (string): Unique identifier (e.g., \"anthropic\", \"openai\")\n- company_name (string): Display name (e.g., \"Anthropic\", \"OpenAI\")\n- domain (string): Company website domain for job search (e.g., \"anthropic.com\")\n\nExample data:\n[\n  { company_id: \"anthropic\", company_name: \"Anthropic\", domain: \"anthropic.com\" },\n  { company_id: \"openai\", company_name: \"OpenAI\", domain: \"openai.com\" }\n]\n\nOutput: Array of company objects, one per table row\n\nProduction notes:\n- For testing: Use 1-3 companies to validate workflow\n- For production: Can handle 40+ companies efficiently\n- Companies processed sequentially (fault tolerance)\n\nArchitectural note:\nThis node loads companies in PARALLEL with profile loading at Start trigger.\nBoth inputs merge in the Merge Data node before the company loop begins.\n\nNext node: Merge Data (combines with profile)\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 3: MERGE COMPANIES WITH PROFILE DATA\n// Purpose: Combines company list with candidate profile for processing\n// Execution Mode: Run Once for All Items\n// ============================================================================\n\n// ARCHITECTURAL SIGNIFICANCE:\n// This is where profile externalization happens. Instead of hardcoding profile\n// data in the workflow, we load it once at the top and merge it with each\n// company. This creates a \"company + profile\" context that flows through the\n// entire workflow.\n\n// STEP 1: Get companies by referencing the Load Companies node\n// .all() returns array of ALL items from that node\nconst companies = $('Load Companies').all();\n\n// STEP 2: Get profile by referencing the Start trigger node\n// .first() gets the first (and only) item from Start trigger\n// The profile data is provided via trigger execution context\nconst profile = $('Start').first().json;\n\n// DEBUGGING: Log what we received (visible in execution log)\nconsole.log('Companies loaded:', companies.length);\nconsole.log('Profile ID:', profile.profile_id);\n\n// STEP 3: Create merged output - one item per company, each with profile data\n// .map() transforms each company into a combined company+profile object\nreturn companies.map(item => ({\n  json: {\n    // === COMPANY FIELDS ===\n    // These identify which company we're processing\n    company_id: item.json.company_id,      // \"anthropic\"\n    company_name: item.json.company_name,  // \"Anthropic\"\n    domain: item.json.domain,              // \"anthropic.com\"\n    \n    // === PROFILE FIELDS ===\n    // These provide candidate context for job matching\n    profile_id: profile.profile_id,              // \"ted_beatie_pm\"\n    profile_name: profile.profile_name,          // \"Ted Beatie - Technical PM\"\n    resume_text: profile.resume_text,            // Full resume/experience (large text)\n    target_criteria: profile.target_criteria,    // JSON string with search criteria\n    notes: profile.notes                         // Additional notes (optional)\n  }\n}));\n\n// OUTPUT: Array of merged objects, one per company\n// Example for 3 companies:\n// [\n//   { company_id: \"anthropic\", domain: \"anthropic.com\", profile_id: \"ted_beatie_pm\", resume_text: \"...\", ... },\n//   { company_id: \"openai\", domain: \"openai.com\", profile_id: \"ted_beatie_pm\", resume_text: \"...\", ... },\n//   { company_id: \"google\", domain: \"google.com\", profile_id: \"ted_beatie_pm\", resume_text: \"...\", ... }\n// ]\n\n// WHY MERGE HERE?\n// 1. LOAD ONCE: Profile loaded once at top, not in loop (efficient)\n// 2. CONTEXT PRESERVATION: Every company carries full profile context\n// 3. CLEAN ARCHITECTURE: Loop Over Companies receives complete context per iteration\n// 4. SIMPLIFIES DOWNSTREAM: No need to reference profile node in loop\n\n// DATA FLOW:\n// This merged data flows to:\n// 1. Build Request → Extracts target_criteria for Apify search\n// 2. Loop Over Companies → Each iteration has company + profile\n// 3. Add Context → Passes profile fields to sub-workflow\n// 4. Loop Jobs → Uses resume_text for AI evaluation\n\n// ARCHITECTURAL IMPROVEMENT:\n// OLD (v5): Profile hardcoded in \"Add Resume Context\" node in Loop Jobs\n// NEW (v2.1): Profile loaded at top-level, flows through entire pipeline\n// BENEFIT: No personal data in workflow JSON, easier updates, multi-user ready\n\n// Next node: Build Request\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -656,
        160
      ],
      "id": "631f4777-1aed-4517-b954-1cbc6637b937",
      "name": "Merge Data"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 4: BUILD APIFY REQUEST FROM MERGED DATA\n// Purpose: Transform company+profile data into Apify API request format\n// Execution Mode: Run Once for All Items\n// ============================================================================\n\n// ARCHITECTURAL CHANGE (v2.1):\n// This node now EXTRACTS target criteria from the profile's target_criteria field\n// instead of hardcoding search parameters. This makes job search criteria\n// configurable per profile.\n\n// STEP 1: Get all merged company+profile items\nconst items = $input.all();\n\n// STEP 2: Transform each item into Apify API request format\nreturn items.map(item => {\n  const data = item.json;\n  \n  // Parse the target criteria from profile data\n  // target_criteria is stored as JSON string in profile, so we parse it\n  // Expected structure:\n  // {\n  //   titleSearch: [\"product manager\"],\n  //   titleExclusionSearch: [\"product marketing\"],\n  //   ... other criteria ...\n  // }\n  const criteria = JSON.parse(data.target_criteria);\n  \n  return {\n    json: {\n      // === APIFY API SEARCH PARAMETERS ===\n      // These fields tell Apify what jobs to find\n      \n      domainFilter: [data.domain],\n      // Company website to search (must be array format)\n      // Example: [\"anthropic.com\"]\n      \n      limit: 10,\n      // Maximum number of jobs to return per company\n      // Increase for more jobs, decrease for faster/cheaper searches\n      \n      timeRange: \"7d\",\n      // How far back to search for jobs\n      // Options: \"1h\", \"24h\", \"7d\", \"30d\", \"3m\", \"6m\"\n      // PRODUCTION: \"7d\" for weekly coverage\n      // TESTING: \"6m\" to ensure jobs are found\n      \n      titleSearch: criteria.titleSearch,\n      // EXTRACTED FROM PROFILE: Only return jobs with these words in title\n      // Example: [\"product manager\"] from profile target_criteria\n      // Case-insensitive search\n      \n      titleExclusionSearch: criteria.titleExclusionSearch,\n      // EXTRACTED FROM PROFILE: Exclude jobs with these words in title\n      // Example: [\"product marketing\"] from profile target_criteria\n      // Helps filter out Product Marketing Manager roles\n      \n      aiWorkArrangementFilter: [\"Hybrid\", \"Remote OK\", \"Remote Solely\"],\n      // Only return jobs that are Remote or Hybrid\n      // Filters out on-site only positions\n      // Note: Could be moved to profile target_criteria in future\n      \n      // === APIFY FEATURE FLAGS ===\n      includeAi: true,\n      // Enable Apify's AI analysis (extracts salary, experience level, etc.)\n      \n      includeLinkedIn: true,\n      // Search LinkedIn jobs in addition to company career pages\n      \n      populateAiRemoteLocationDerived: true,\n      // Extract and standardize remote work locations\n      \n      descriptionType: \"text\"\n      // Return job descriptions as plain text (alternative: \"html\")\n      \n      // NOTE: We do NOT include _context_ fields here\n      // Those are added later by the \"Add Context\" node\n      // This keeps the Apify request clean and focused on search parameters\n    }\n  };\n});\n\n// WHAT GETS OUTPUT:\n// Array of Apify request objects, one per company\n// Example for 2 companies:\n// [\n//   { domainFilter: [\"anthropic.com\"], limit: 10, titleSearch: [\"product manager\"], ... },\n//   { domainFilter: [\"openai.com\"], limit: 10, titleSearch: [\"product manager\"], ... }\n// ]\n\n// ARCHITECTURAL IMPROVEMENT:\n// OLD (v5): Search parameters hardcoded in this node\n// NEW (v2.1): Search parameters extracted from profile.target_criteria\n// BENEFIT: Different users can have different search criteria\n//          Update criteria by changing profile, not workflow\n\n// PROFILE INTEGRATION:\n// The profile data (resume_text, etc.) is NOT included in Apify request.\n// It's preserved in the workflow context and added later by Add Context node.\n// This separation keeps concerns clean:\n// - Build Request: Apify search configuration\n// - Add Context: Workflow tracking and sub-workflow context\n\n// Next node: Loop Over Companies\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -432,
        160
      ],
      "id": "7b32ecc7-ad8b-4250-90de-c9e39b11087d",
      "name": "Build Request"
    },
    {
      "parameters": {
        "options": {}
      },
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [
        -224,
        160
      ],
      "id": "7e92263b-c54d-4593-8505-a9caf597bd69",
      "name": "Loop Over Companies",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 5: LOOP OVER COMPANIES (COMPANY WORKFLOW ORCHESTRATOR)\n═══════════════════════════════════════════════════════════════════════════\nProcesses companies ONE AT A TIME using a loop pattern.\n\nWhat is a loop node?\n- Takes an array of items (companies) and processes them one by one\n- After processing one company completely, automatically advances to next\n- Batch size: 1 (default) means process one company per loop iteration\n\nWhy process one company at a time instead of all at once?\n1. FAULT TOLERANCE: If company #5 fails, companies #6-40 still process\n2. INCREMENTAL SAVES: Results saved to database after each company\n3. PROGRESS TRACKING: Can monitor real-time progress in n8n UI\n4. COST CONTROL: Know exact API call counts and costs\n5. CLEAN SEPARATION: Each company's jobs are processed as a complete unit\n   before moving to the next company\n\nHow the loop works:\n┌─ Loop iteration 1: Process Company 1 (Anthropic)\n│  ├─ Call Apify → Get 8 jobs\n│  ├─ Save raw jobs to database (fast path)\n│  ├─ Call Loop Jobs sub-workflow → Process all 8 jobs (slow path)\n│  └─ Return to this loop node\n│\n├─ Loop iteration 2: Process Company 2 (OpenAI)\n│  ├─ Call Apify → Get 5 jobs\n│  ├─ Save raw jobs to database (fast path)\n│  ├─ Call Loop Jobs sub-workflow → Process all 5 jobs (slow path)\n│  └─ Return to this loop node\n│\n└─ All companies done → Done branch activates\n\nTwo outputs (pins on the node):\n1. DONE BRANCH (top pin):\n   - Fires ONCE after ALL companies complete\n   - Routes to: Workflow Summary\n   - Used for triggering email workflow\n   \n2. LOOP BRANCH (bottom pin):\n   - Fires ONCE PER COMPANY\n   - Routes to: Run Apify\n   - This is where the work happens\n\nAccessing current company in downstream nodes:\n$('Loop Over Companies').item.json\n// This gives you the current company being processed\n// Includes BOTH company fields AND profile fields from merge:\n// {\n//   company_id: \"anthropic\",\n//   company_name: \"Anthropic\",\n//   domain: \"anthropic.com\",\n//   profile_id: \"ted_beatie_pm\",\n//   resume_text: \"...\",\n//   target_criteria: \"{...}\",\n//   ...\n// }\n\nKey difference from v5 architecture:\n- OLD: Nested dual-loop (company loop containing job loop)\n- NEW: Single company loop that calls separate Job workflow\n- BENEFIT: Cleaner separation, easier debugging, reusable job processing\n\nContext preservation:\nEach loop iteration has access to the merged company+profile data.\nThis means downstream nodes (Add Context, Save Apify data, etc.) can\nreference both company fields and profile fields without additional lookups.\n\nNext nodes:\n- Loop branch → Run Apify (process current company)\n- Done branch → Workflow Summary (prepare for email workflow)\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "operation": "Run actor and get dataset",
        "actorId": {
          "__rl": true,
          "value": "s3dtSTZSZWFtAVLn5",
          "mode": "list",
          "cachedResultName": "Career Site Job Listing API (fantastic-jobs/career-site-job-listing-api)",
          "cachedResultUrl": "https://console.apify.com/actors/s3dtSTZSZWFtAVLn5/input"
        },
        "customBody": "={{ $json }}",
        "timeout": {},
        "authentication": "apifyOAuth2Api"
      },
      "type": "@apify/n8n-nodes-apify.apify",
      "typeVersion": 1,
      "position": [
        0,
        176
      ],
      "id": "6c8129af-24d5-429f-9a44-480c8ebba6b0",
      "name": "Run Apify",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 6: CALL APIFY JOB SEARCH API\n═══════════════════════════════════════════════════════════════════════════\nCalls Apify's Career Site Job Listing API to search for jobs.\n\nWhat is Apify?\n- Third-party job search service that crawls company career pages\n- Aggregates jobs from ATS platforms (Greenhouse, Lever, Workday, etc.)\n- Includes AI-powered extraction of salary, location, experience level\n\nWhat this node does:\n1. Receives ONE company request (from loop)\n2. Calls Apify API with search parameters from Build Request node\n3. Returns 0-10 job listings for that company\n\nCritical settings explained:\n- alwaysOutputData: true\n  → Ensures node outputs data even if 0 jobs found\n  → Without this, 0 jobs would skip downstream nodes\n  \n- onError: continueErrorOutput\n  → If API fails, route to error branch instead of crashing workflow\n  → Enables fault tolerance\n  \n- customBody: ={{ $json }}\n  → Send ALL fields from previous node (Build Request) to API\n  → Includes Apify search parameters (domainFilter, titleSearch, etc.)\n\nTwo outputs (important!):\n1. SUCCESS BRANCH (top pin):\n   - API call succeeded (even if 0 jobs returned)\n   - Routes to: IF node (check if jobs exist)\n   \n2. ERROR BRANCH (bottom pin):\n   - API call failed (network error, timeout, authentication failure)\n   - Routes to: Log Error node\n\nExample scenarios:\n┌─ Anthropic: Returns 8 jobs → Success branch with 8 items\n├─ Fictional company: Returns 0 jobs → Success branch with 0 items  \n└─ API timeout: → Error branch with error object\n\nAutomatic iteration:\nIf Apify returns 8 jobs, n8n AUTOMATICALLY processes each job\nindividually through downstream nodes. No explicit job loop needed here.\n\nEach job item includes:\n- Basic info: id, title, organization, url\n- Location: cities_derived, regions_derived, remote_derived\n- Salary: ai_salary_minvalue, ai_salary_maxvalue, ai_salary_currency\n- Details: description_text, ai_experience_level, ai_work_arrangement\n\nNote about profile data:\nThe Apify response does NOT include profile data. That's preserved in the\nLoop Over Companies iteration context and will be added by Add Context node.\n\nCost:\n- ~$4 per 1,000 jobs fetched from Apify\n- For 40 companies × avg 3 jobs = 120 jobs/day = ~$0.48/day\n\nNext nodes:\n- Success → IF node (check if jobs exist)\n- Error → Log Error node\n═══════════════════════════════════════════════════════════════════════════",
      "alwaysOutputData": true,
      "credentials": {
        "apifyOAuth2Api": {
          "id": "AVesD0GKdfEOo8qV",
          "name": "Apify account"
        }
      },
      "onError": "continueErrorOutput"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 3
          },
          "conditions": [
            {
              "id": "a2ed7fda-9f49-4751-838b-b07648ec9cf3",
              "leftValue": "={{ !!($json && ($json.id || $json.title || $json.organization)) }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.3,
      "position": [
        256,
        160
      ],
      "id": "34784c54-8305-45e2-841d-99cd04767a25",
      "name": "If",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 7: CHECK IF JOBS WERE FOUND\n═══════════════════════════════════════════════════════════════════════════\nDetermines if Apify returned actual job data or empty results.\n\nThe problem we're solving:\n- When Apify finds 0 jobs, it might return empty items or no items\n- We need to distinguish between \"has jobs\" vs \"no jobs\" to route correctly\n\nCondition explained in detail:\n{{ !!($json && ($json.id || $json.title || $json.organization)) }}\n\nLet's break this down:\n\n1. $json\n   - The current item from Run Apify node\n   - Could be a real job OR an empty item\n\n2. $json.id || $json.title || $json.organization\n   - Checks if ANY of these fields exist\n   - Real jobs always have at least one of these\n   - Empty items have none\n   - Returns: the field value (truthy) OR undefined (falsy)\n\n3. !!\n   - Double negation - converts ANY value to boolean (true/false)\n   - Why? Because IF node needs true/false, not a string or undefined\n   - \"1234\" → !!\"1234\" → true\n   - undefined → !!undefined → false\n\nExamples:\n- Real job: {id: \"1234\", title: \"Product Manager\", ...} → TRUE\n- Empty item: {} → FALSE\n- Missing item: → FALSE\n\nTwo branches (ARCHITECTURE CHANGE FROM v5):\n\n1. TRUE BRANCH (jobs found):\n   Routes to: Add Context node\n   \n   Add Context then splits into TWO PARALLEL paths:\n   \n   Path A: Save Apify data → Insert row (ends cleanly)\n   - Saves raw job data to database immediately\n   - Fast path - completes in ~100ms per job\n   - Does NOT loop back or call sub-workflow\n   - Purpose: Backup and searchable job storage\n   \n   Path B: Call 'Loop Jobs' sub-workflow → loops back to company loop\n   - Calls separate \"Loop Jobs\" workflow to process each job\n   - That workflow handles: AI evaluation, merge, format, email queue\n   - Slow path - takes 2-5 seconds per job\n   - DOES loop back when ALL jobs for current company complete\n   - Purpose: AI-powered job matching and email preparation\n   \n   WHY TWO PATHS?\n   - Path A preserves all data quickly (backup + immediate availability)\n   - Path B does expensive AI processing in separate workflow\n   - Separation prevents workflow complexity and enables independent testing\n\n2. FALSE BRANCH (no jobs found):\n   → Log no results → Insert bad row → Loop back\n   - Creates error record in database\n   - Allows company loop to continue\n\nImportant behavior:\nIf 8 jobs found, this IF node evaluates 8 TIMES (once per job).\nEach job flows through Add Context, then splits to both paths.\n\nArchitectural improvement from v5:\n- OLD: Three parallel paths with merge complexity\n- NEW: Two parallel paths, no merge needed (handled in sub-workflow)\n\nNext nodes:\n- TRUE → Add Context (then splits to Save + Call sub-workflow)\n- FALSE → Log no results\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 8: ADD CONTEXT TO JOBS\n// Purpose: Enriches Apify jobs with parent workflow context AND profile data\n// ============================================================================\n\n// ARCHITECTURAL CHANGE (v2.1):\n// This node now adds PROFILE CONTEXT in addition to company context.\n// The profile fields enable the Loop Jobs sub-workflow to perform AI evaluation\n// without needing to load profile data separately.\n\n// STEP 1: Get all jobs returned from Apify for current company\nconst jobs = $input.all().map(item => item.json);\n\n// STEP 2: Get the current iteration's company+profile data from Merge Data node\n// This contains the merged data BEFORE it was transformed into Apify request format\n// We need the original merged data because it has profile fields that were\n// stripped out by Build Request (Apify doesn't need them)\nconst currentIteration = $('Merge Data').item.json;\n\n// WHY REFERENCE MERGE DATA INSTEAD OF LOOP OVER COMPANIES?\n// Loop Over Companies has the BUILD REQUEST output (Apify search params only)\n// Merge Data has the ORIGINAL merged company+profile data\n// We need the latter for profile fields (resume_text, target_criteria, etc.)\n\n// STEP 3: Attach context to each job\n// This creates a comprehensive context bundle that includes:\n// - All original Apify job fields\n// - Company identification\n// - Profile data for AI evaluation\n// - Workflow tracking IDs\nreturn jobs.map(job => ({\n  json: {\n    // === PRESERVE ALL ORIGINAL APIFY JOB FIELDS ===\n    // The spread operator (...job) copies ALL fields from the job\n    // This includes: id, title, description, salary, location, etc.\n    ...job,\n    \n    // === PARENT WORKFLOW TRACKING ===\n    // This workflow's execution ID - used to group all jobs from this run\n    _context_workflow_run_id: $execution.id,\n    \n    // === COMPANY CONTEXT ===\n    // Identifies which company posted this job\n    _context_company_id: currentIteration.company_id,\n    _context_company_name: currentIteration.company_name,\n    _context_domain: currentIteration.domain,\n    \n    // === PROFILE CONTEXT (NEW IN v2.1) ===\n    // Provides candidate profile data for AI evaluation in Loop Jobs\n    _context_profile_id: currentIteration.profile_id,\n    _context_resume_text: currentIteration.resume_text,\n    _context_target_criteria: currentIteration.target_criteria\n  }\n}));\n\n// OUTPUT: Jobs with comprehensive context\n// Example structure for one job:\n// {\n//   // Original Apify fields:\n//   id: \"job_12345\",\n//   title: \"Senior Product Manager\",\n//   description_text: \"We are looking for...\",\n//   ai_salary_minvalue: 200000,\n//   ai_salary_maxvalue: 250000,\n//   ... (all other Apify fields),\n//   \n//   // Context fields:\n//   _context_workflow_run_id: 67890,\n//   _context_company_id: \"anthropic\",\n//   _context_company_name: \"Anthropic\",\n//   _context_domain: \"anthropic.com\",\n//   _context_profile_id: \"ted_beatie_pm\",\n//   _context_resume_text: \"TED BEATIE - PRODUCT LEADER...\" (full resume),\n//   _context_target_criteria: \"{\\\"titleSearch\\\": [\\\"product manager\\\"], ...}\"\n// }\n\n// WHY ADD PROFILE CONTEXT HERE?\n// 1. CROSS-WORKFLOW COMMUNICATION: Sub-workflow needs profile for AI evaluation\n// 2. SINGLE BUNDLE: All data needed for job evaluation in one package\n// 3. NO ADDITIONAL LOOKUPS: Loop Jobs doesn't need to query profile database\n// 4. CONSISTENCY: Each job carries identical profile context (no race conditions)\n\n// DATA FLOW:\n// These enriched jobs flow through TWO parallel paths:\n// Path A: Save Apify data → Database (preserves _context_ fields in job_data_full)\n// Path B: Call Loop Jobs → AI evaluation (uses _context_resume_text)\n\n// ARCHITECTURAL IMPROVEMENT:\n// OLD (v5): Profile hardcoded in Loop Jobs workflow\n// NEW (v2.1): Profile passed as context from parent workflow\n// BENEFIT: Loop Jobs workflow is now profile-agnostic (can work with any profile)\n\n// Next nodes (PARALLEL):\n// - Save Apify data (fast path - backup to database)\n// - Call 'Loop Jobs' sub-workflow (slow path - AI evaluation)\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        448,
        112
      ],
      "id": "6f31591f-302e-410b-83bd-740673a4888a",
      "name": "Add Context"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 9: SAVE RAW JOB DATA\n// Extracts key fields from Apify jobs for database storage\n// ============================================================================\n\n// PURPOSE:\n// This is the \"fast path\" - immediate backup of all job data.\n// Runs in PARALLEL with the AI evaluation sub-workflow.\n// Completes in ~100ms while AI evaluation takes 2-5 seconds per job.\n\n// STEP 1: Get all jobs for current company (with context already added)\nconst currentJobs = $input.all().map(item => item.json);\n\n// STEP 2: Get current company context from loop\n// We need this to populate company-related fields in the database\n// Note: We use Loop Over Companies here (not Merge Data) because we only\n// need company identification fields, not the full profile data\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// STEP 3: Transform each job into database row format\n// Creates one database row per job with extracted searchable fields\nreturn currentJobs.map(job => ({\n  json: {\n    // === COMPANY CONTEXT ===\n    // Links this job back to the company that posted it\n    company_id: currentCompany.company_id,\n    company_name: currentCompany.company_name,\n    domain_guess: currentCompany.domain,\n    \n    // === JOB IDENTIFIERS ===\n    job_id: job.id || '',  // Apify's unique job ID\n    job_title: job.title || '',  // \"Senior Product Manager\"\n    organization: job.organization || '',  // \"Anthropic\"\n    organization_url: job.organization_url || '',  // Link to company page\n    \n    // === DATES ===\n    date_posted: job.date_posted || null,  // When job was posted\n    date_created: job.date_created || null,  // When Apify first found it\n    \n    // === LOCATION ===\n    // Extract first location from locations_raw array\n    // The ?. is \"optional chaining\" - prevents errors if field doesn't exist\n    location_locality: job.locations_raw?.[0]?.address?.addressLocality || '',  // City\n    location_region: job.locations_raw?.[0]?.address?.addressRegion || '',  // State\n    location_country: job.locations_raw?.[0]?.address?.addressCountry || '',  // Country\n    remote_derived: job.remote_derived || false,  // Is it remote?\n    \n    // === SALARY (AI-EXTRACTED BY APIFY) ===\n    salary_currency: job.ai_salary_currency || '',  // \"USD\"\n    salary_min: job.ai_salary_minvalue || null,  // 200000\n    salary_max: job.ai_salary_maxvalue || null,  // 250000\n    salary_unit: job.ai_salary_unittext || '',  // \"YEAR\"\n    \n    // === JOB DETAILS ===\n    // Convert array to comma-separated string if needed\n    employment_type: Array.isArray(job.employment_type)\n      ? job.employment_type.join(', ')  // [\"FULL_TIME\"] → \"FULL_TIME\"\n      : '',\n    experience_level: job.ai_experience_level || '',  // \"5-10\" years\n    work_arrangement: job.ai_work_arrangement || '',  // \"Hybrid\", \"Remote\"\n    \n    // === SOURCE/ATS PLATFORM ===\n    source_ats: job.source || '',  // \"greenhouse\", \"lever\", etc.\n    source_domain: job.source_domain || '',  // \"jobs.lever.co\"\n    domain_derived: job.domain_derived || '',  // Company's actual domain\n    \n    // === LINKS ===\n    job_url: job.url || '',  // Direct link to apply\n    \n    // === FULL JSON BACKUP ===\n    // Store the COMPLETE job object as JSON string\n    // This preserves EVERYTHING including _context_ fields\n    // IMPORTANT: This includes profile data from Add Context node!\n    // The _context_resume_text and other profile fields are preserved here\n    job_data_full: JSON.stringify(job)\n  }\n}));\n\n// WHAT GETS OUTPUT:\n// One database row per job with extracted fields\n// Example for 8 jobs: 8 separate database rows\n\n// WHY WE DO THIS:\n// 1. SEARCHABLE FIELDS: Can query by salary, location, title without parsing JSON\n// 2. CSV-FRIENDLY: All fields are simple strings/numbers\n// 3. COMPLETE BACKUP: job_data_full preserves everything including _context_ fields\n// 4. PERFORMANCE: Don't need to parse JSON for common queries\n// 5. IMMEDIATE AVAILABILITY: Jobs visible in database while AI evaluation runs\n// 6. PROFILE PRESERVATION: job_data_full includes _context_resume_text and other profile fields\n\n// ARCHITECTURAL NOTE:\n// This is the \"fast path\" that runs PARALLEL to AI evaluation.\n// In v5, this path looped back. In the new architecture:\n// - This path: Saves and ENDS (no loop back)\n// - Other path: Calls sub-workflow which loops back when done\n// This prevents race conditions and simplifies flow control.\n\n// Next node: Insert row (saves to database, then ends)\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        624,
        32
      ],
      "id": "4e854b0f-5929-40f3-86fc-f88a2ed5e31c",
      "name": "Save Apify data"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "SE89PRGWhcrR39Ar",
          "mode": "list",
          "cachedResultName": "jobs test 1",
          "cachedResultUrl": "/projects/MIniGOtG65wFTuKC/datatables/SE89PRGWhcrR39Ar"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "company_id": "={{ $json.company_id }}",
            "domain_derived": "={{ $json.domain_derived }}",
            "company_name": "={{ $json.company_name }}",
            "job_id": "={{ $json.job_id }}",
            "date_posted": "={{ $json.date_posted }}",
            "job_title": "={{ $json.job_title }}",
            "locality": "={{ $json.location_locality }}",
            "remote_derived": "={{ $json.remote_derived }}",
            "region": "={{ $json.location_region }}",
            "salary_min": "={{ $json.salary_min }}",
            "job_url": "={{ $json.job_url }}",
            "salary_max": "={{ $json.salary_max }}",
            "job_data": "={{ $json.job_data_full }}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "company_id",
              "displayName": "company_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "company_name",
              "displayName": "company_name",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domain_derived",
              "displayName": "domain_derived",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_id",
              "displayName": "job_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_title",
              "displayName": "job_title",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "date_posted",
              "displayName": "date_posted",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "locality",
              "displayName": "locality",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "region",
              "displayName": "region",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "remote_derived",
              "displayName": "remote_derived",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "salary_min",
              "displayName": "salary_min",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "salary_max",
              "displayName": "salary_max",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_url",
              "displayName": "job_url",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "job_data",
              "displayName": "job_data",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        864,
        32
      ],
      "id": "0bd28ba5-0c0c-44a9-9602-7f485e983803",
      "name": "Insert row",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 10: INSERT RAW JOBS TO DATABASE (FAST PATH)\n═══════════════════════════════════════════════════════════════════════════\nInserts raw job data into the 'jobs_test_1' database table.\n\nWhat this node does:\n- Takes formatted job data from Save Apify data node\n- Inserts ONE ROW PER JOB into database\n- Completes quickly (~100ms per job) while AI evaluation runs in parallel\n\nWhy save raw jobs to database?\n1. IMMEDIATE BACKUP: Preserves all job data before any processing\n2. SEARCHABLE: Can query by salary, location, company without AI evaluation\n3. HISTORICAL RECORD: Track jobs over time, even if AI evaluation fails\n4. DEBUGGING: Compare raw data vs AI evaluation results\n5. FAIL-SAFE: If sub-workflow fails, we still have the raw job data\n6. PROFILE BACKUP: job_data_full includes _context_ profile fields\n\nExample: If Anthropic has 8 jobs, this node executes 8 times,\ncreating 8 separate database rows.\n\nField mapping explanation:\n={{ $json.company_id }} means \"read company_id field from input data\"\n\nDatabase columns stored:\n- company_id, company_name, domain_derived: Company identification\n- job_id, job_title, job_url: Job identifiers and links\n- locality, region, remote_derived: Location information\n- salary_min, salary_max: Compensation range\n- job_data: Complete JSON backup (includes _context_ fields with profile data)\n\nParallel execution:\nThis node runs in PARALLEL with \"Call 'Loop Jobs'\" sub-workflow.\n- This path: Fast storage, completes in milliseconds\n- Other path: AI evaluation, takes 2-5 seconds per job\n- Both paths must complete before company loop advances\n\nArchitectural change from v5:\nOLD: This path looped back to inner job loop\nNEW: This path ENDS after insert (no loop back)\n     The sub-workflow path handles loop-back to company loop\n     This simplifies flow control and prevents race conditions\n\nCritical: This path does NOT loop back!\n- Saves data and ends cleanly\n- The \"Call 'Loop Jobs'\" path handles the loop-back\n- This prevents timing issues and simplifies debugging\n\nNote about profile data:\nThe job_data column contains the full JSON including _context_resume_text\nand other profile fields. This means the raw job backup preserves the\ncandidate profile that was used for evaluation context.\n\nNext: [No connection - path ends here]\n       Workflow waits for parallel \"Call 'Loop Jobs'\" path to complete\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "value": "TqY18KRTpfKplhh0",
          "mode": "list",
          "cachedResultUrl": "/workflow/TqY18KRTpfKplhh0",
          "cachedResultName": "Loop Jobs v3.1"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {},
          "matchingColumns": [],
          "schema": [],
          "attemptToConvertTypes": false,
          "convertFieldsToString": true
        },
        "options": {}
      },
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.3,
      "position": [
        1280,
        400
      ],
      "id": "989f482f-4572-4a9d-866b-f935d061de1b",
      "name": "Call 'Loop Jobs v3.1'",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 11: CALL SUB-WORKFLOW (EXECUTE WORKFLOW)\n═══════════════════════════════════════════════════════════════════════════\nCalls the separate \"Loop Jobs\" workflow to process each job with AI evaluation.\n\nWhat is Execute Workflow?\n- n8n node type that calls another workflow\n- Passes data across workflow boundary\n- Sub-workflow runs in separate execution context\n- Returns control when sub-workflow completes\n\nWhat this node does:\n1. Receives jobs with context from \"Add Context\" node\n2. Calls \"Loop Jobs v3.1\" workflow\n3. Passes ALL job data (including _context_ fields) to sub-workflow\n4. Waits for sub-workflow to complete ALL jobs for current company\n5. Returns to company loop when done\n\nSub-workflow (Loop Jobs v3.1) handles:\n- Individual job iteration (loops through jobs one at a time)\n- AI evaluation with Claude Sonnet 4.5 using _context_resume_text\n- Parsing AI response\n- Merging AI evaluation with raw job data\n- Formatting HTML job cards\n- Saving to email queue table\n\nWhy separate workflows?\n1. SEPARATION OF CONCERNS:\n   - Company workflow: Orchestration and company-level operations\n   - Job workflow: Individual job processing and AI evaluation\n   \n2. INDEPENDENT TESTING:\n   - Can test job evaluation without running company loop\n   - Can test company discovery without AI evaluation\n   \n3. EASIER DEBUGGING:\n   - Isolate issues to specific workflow\n   - View execution logs separately\n   \n4. REUSABILITY:\n   - Job workflow could be triggered from other sources\n   - E.g., manual job evaluation, webhook triggers\n   \n5. CLEANER ARCHITECTURE:\n   - No nested loops in single workflow\n   - Clear workflow boundaries and responsibilities\n\nData flow across workflow boundary:\nParent (this workflow) passes to sub-workflow:\n- All Apify job fields (id, title, salary, location, description, etc.)\n- _context_workflow_run_id (parent execution ID for grouping)\n- _context_company_id (company identifier)\n- _context_company_name (company display name)\n- _context_domain (company domain)\n- _context_profile_id (candidate profile identifier) - NEW IN v2.1\n- _context_resume_text (full resume for AI evaluation) - NEW IN v2.1\n- _context_target_criteria (job search criteria JSON) - NEW IN v2.1\n\nARCHITECTURAL CHANGE (v2.1):\nOLD: Loop Jobs had hardcoded resume in \"Add Resume Context\" node\nNEW: Loop Jobs receives resume via _context_resume_text from parent\nBENEFIT: Loop Jobs is now profile-agnostic, works with any profile\n\nSub-workflow returns:\nNothing explicit - it saves directly to email queue table.\nThe parent workflow continues when sub-workflow execution completes.\n\nTiming:\n- If 8 jobs found for Anthropic\n- This node is called with all 8 jobs\n- Sub-workflow processes them one-by-one (internal loop)\n- Takes 2-5 seconds per job = 16-40 seconds total\n- Returns control after all 8 jobs processed\n\nParallel execution:\nThis node runs in PARALLEL with \"Save Apify data\" path:\n- Save path: Completes in milliseconds\n- This path: Completes in seconds (AI evaluation)\n- Company loop waits for BOTH to complete\n\nArchitectural change from v5:\nOLD: \"Loop Over Jobs\" node (internal nested loop)\n     - AI evaluation in same workflow\n     - Merge nodes, complex parallel paths\n     - Hard to debug and test independently\n     - Profile hardcoded in workflow\n     \nNEW: \"Call 'Loop Jobs'\" node (sub-workflow call)\n     - AI evaluation in separate workflow\n     - Cleaner separation of concerns\n     - Independent testing and debugging\n     - Profile passed via context\n     - Same end result, better architecture\n\nCritical: This path DOES loop back!\n- After sub-workflow completes all jobs for current company\n- Loops back to \"Loop Over Companies\" node\n- Company loop advances to next company\n\nLoop-back path:\nCall 'Loop Jobs' → Loop Over Companies → (next company iteration)\n\nNext node: Loop Over Companies (loops back for next company)\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 12: LOG NO RESULTS\n// Handles successful API calls that returned 0 jobs\n// ============================================================================\n\n// What this node does:\n// When Apify successfully calls a company but finds 0 matching jobs,\n// we still want to track this (so we know the company was checked).\n\n// Get current company from the company loop\n// Note: We can access both company and profile fields from the loop context\n// because they were merged in the Merge Data node\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// Create error record for database\nreturn [{\n  json: {\n    company_id: currentCompany.company_id,\n    company_name: currentCompany.company_name,\n    domain_guess: currentCompany.domain,\n    \n    status: 'no_results',  // Indicates API worked but 0 jobs found\n    jobs_found: 0,\n    \n    // No domain verification data (not applicable without jobs)\n    domain_verified: null,\n    domains_seen: '',\n    ats_platforms_seen: '',\n    ats_type: '',\n    \n    // Explanation\n    note: 'API succeeded but returned 0 jobs - company not found in Apify or no matching PM roles',\n    \n    // Timestamp\n    verified_at: new Date().toISOString()\n  }\n}];\n\n// WHY THIS MATTERS:\n// - Distinguishes between \"no jobs\" (tracked here) and \"API error\" (Log Error)\n// - Helps identify companies to remove or update in companies table\n// - Tracks coverage (which companies we're monitoring)\n// - Provides audit trail of workflow execution\n\n// Possible reasons for 0 jobs:\n// 1. Company not currently hiring PMs\n// 2. Company name mismatch (\"OpenAI\" vs \"Open AI\" vs \"OpenAI Inc\")\n// 3. Company not in Apify database\n// 4. Our filters too restrictive (title, remote, timeRange)\n// 5. All PM jobs filled/closed recently\n// 6. Target criteria from profile too specific (titleSearch, etc.)\n\n// PROFILE CONTEXT:\n// Although we have access to profile fields via currentCompany.resume_text,\n// we don't include them in the error log. The error tracking is focused on\n// company-level issues, not profile-specific issues.\n\n// Next node: Insert bad row (saves to errors table)\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        624,
        256
      ],
      "id": "1b1ffd15-aaad-42eb-9a4e-1b89fee44bf0",
      "name": "Log no results"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 13: LOG ERROR\n// Formats Apify API errors for database insertion\n// ============================================================================\n\n// What this node does:\n// When Apify API call completely fails (timeout, auth error, network issue),\n// we log the error so we can debug and retry later.\n\n// Get current company from the company loop\nconst currentCompany = $('Loop Over Companies').item.json;\n\n// Get error details from Apify node's error output\n// The Run Apify node passes error information through $json.error\nconst errorInfo = $json.error || { message: 'Unknown error' };\n\n// Create error record for database\nreturn [{\n  json: {\n    company_id: currentCompany.company_id,\n    company_name: currentCompany.company_name,\n    domain_guess: currentCompany.domain,\n    \n    status: 'api_error',  // Indicates API call failed completely\n    jobs_found: 0,\n    \n    // No domain verification data\n    domain_verified: null,\n    domains_seen: '',\n    ats_platforms_seen: '',\n    ats_type: '',\n    \n    // Include error message for debugging\n    note: `Apify API Error: ${errorInfo.message || JSON.stringify(errorInfo)}`,\n    \n    // Timestamp\n    verified_at: new Date().toISOString()\n  }\n}];\n\n// COMMON ERROR TYPES:\n// - Timeout: API took too long to respond (>30 seconds)\n// - Authentication: API key invalid, expired, or missing\n// - Rate limit: Too many requests in short period\n// - Network: Connection issues, DNS problems, firewall\n// - 500 error: Apify server problem\n// - 400 error: Invalid request parameters (check Build Request node)\n\n// WHAT TO DO WHEN ERRORS OCCUR:\n// 1. Check error message in database after workflow completes\n// 2. Verify Apify credentials in n8n settings\n// 3. Check Apify service status (status.apify.com)\n// 4. Review request parameters in Build Request node\n// 5. Verify target_criteria JSON is valid in profile\n// 6. Re-run workflow for failed companies\n// 7. Contact Apify support if persistent issues\n\n// DEBUGGING TIPS:\n// - View full error in n8n execution log\n// - Check Apify dashboard for account status\n// - Verify domain format in companies table\n// - Test with known-good company (e.g., \"anthropic.com\")\n// - Check if profile.target_criteria is valid JSON\n\n// PROFILE CONTEXT:\n// Although we have access to profile fields, we don't include them in the\n// error log. API errors are typically infrastructure issues, not profile issues.\n\n// Next node: Insert bad row (saves to errors table)\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        256,
        320
      ],
      "id": "4f53dd90-668a-4e05-b3f7-dc55ee2f458f",
      "name": "Log Error"
    },
    {
      "parameters": {
        "dataTableId": {
          "__rl": true,
          "value": "H1hVAfE9VHAFwNZy",
          "mode": "list",
          "cachedResultName": "errors 1",
          "cachedResultUrl": "/projects/MIniGOtG65wFTuKC/datatables/H1hVAfE9VHAFwNZy"
        },
        "columns": {
          "mappingMode": "defineBelow",
          "value": {
            "company_id": "={{ $json.company_id }}",
            "company_name": "={{ $json.company_name }}",
            "domain_guess": "={{ $json.domain_guess }}",
            "domain_verified": "={{ $json.domain_verified }}",
            "domains_seen": "={{ $json.domains_seen }}",
            "ats_platforms_seen": "={{ $json.ats_platforms_seen }}",
            "verified_at": "={{ $json.verified_at }}",
            "note": "={{ $json.note }}",
            "ats_type": "={{ $json.ats_type }}",
            "status": "={{ $json.status }}",
            "jobs_found": "={{ $json.jobs_found }}"
          },
          "matchingColumns": [],
          "schema": [
            {
              "id": "company_id",
              "displayName": "company_id",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "company_name",
              "displayName": "company_name",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domain_guess",
              "displayName": "domain_guess",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domain_verified",
              "displayName": "domain_verified",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "ats_type",
              "displayName": "ats_type",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "status",
              "displayName": "status",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "jobs_found",
              "displayName": "jobs_found",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "domains_seen",
              "displayName": "domains_seen",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "ats_platforms_seen",
              "displayName": "ats_platforms_seen",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "note",
              "displayName": "note",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "string",
              "readOnly": false,
              "removed": false
            },
            {
              "id": "verified_at",
              "displayName": "verified_at",
              "required": false,
              "defaultMatch": false,
              "display": true,
              "type": "dateTime",
              "readOnly": false,
              "removed": false
            }
          ],
          "attemptToConvertTypes": false,
          "convertFieldsToString": false
        },
        "options": {}
      },
      "type": "n8n-nodes-base.dataTable",
      "typeVersion": 1,
      "position": [
        864,
        368
      ],
      "id": "17519208-6c20-4f23-9599-62048973ba05",
      "name": "Insert bad row",
      "COMMENT": "═══════════════════════════════════════════════════════════════════════════\nNODE 14: SAVE ERRORS TO DATABASE\n═══════════════════════════════════════════════════════════════════════════\nInserts error records (no results + API failures) to 'errors 1' table.\n\nWhat this node does:\n- Saves error/no-result records to database for tracking and debugging\n- Receives from TWO sources:\n  1. Log no results (status: 'no_results') - API succeeded but 0 jobs\n  2. Log Error (status: 'api_error') - API call failed completely\n\nWhy track errors in database?\n1. DEBUGGING: Know which companies failed and why\n2. MONITORING: Track API reliability over time\n3. MAINTENANCE: Identify companies to update or remove from monitoring list\n4. COVERAGE: Verify all companies were checked during workflow run\n5. AUDIT TRAIL: Historical record of workflow execution issues\n\nStatus types stored:\n- 'no_results': API succeeded, but 0 jobs found for company\n  - Possible reasons: Not hiring, name mismatch, not in Apify\n  - Could also be due to target_criteria from profile being too restrictive\n- 'api_error': API call completely failed\n  - Possible reasons: Timeout, auth error, network issue, rate limit\n  - Could also be due to invalid target_criteria JSON in profile\n\nAll error types go to the SAME table for centralized tracking.\nThis allows queries like: \"Show me all errors from last week\"\n\nExample error records:\n┌─────────────┬────────────┬────────────────────────┐\n│ company_id  │ status     │ note                   │\n├─────────────┼────────────┼────────────────────────┤\n│ anthropic   │ no_results │ 0 jobs - not hiring    │\n│ fakecorp    │ api_error  │ Timeout after 30s      │\n│ badname     │ no_results │ Name mismatch          │\n└─────────────┴────────────┴────────────────────────┘\n\nQuerying errors after workflow completion:\nSELECT company_name, status, note, jobs_found, verified_at\nFROM errors_1\nWHERE DATE(verified_at) = CURRENT_DATE\nORDER BY status, company_name;\n\nProduction monitoring:\n- Schedule daily query to check error count\n- Alert if error rate exceeds threshold (e.g., >10%)\n- Review no_results companies monthly\n- Update company names or remove inactive companies\n- Check if target_criteria needs adjustment for profile\n\nProfile context:\nAlthough errors are tracked at company level, remember that the workflow\nis now profile-aware. If many companies show no_results, consider checking\nif profile.target_criteria is too restrictive (e.g., titleSearch too specific).\n\nNext node: Loop Over Companies (loops back for next company)\n═══════════════════════════════════════════════════════════════════════════"
    },
    {
      "parameters": {
        "jsCode": "// ============================================================================\n// NODE 15: PREPARE FOR EMAIL QUERY\n// Purpose: Convert multiple loop items into single execution context\n// ============================================================================\n\n// THE PROBLEM:\n// The Done branch from Loop Over Companies receives multiple items\n// (one item per company processed). Each item is the merged company+profile data.\n// But we only need to query the database ONCE using the workflow run ID.\n\n// THE SOLUTION:\n// Consolidate all those items into a SINGLE item with summary info.\n// This prevents the email workflow from being called multiple times.\n\n// Get all items that came through the loop (all companies)\nconst allCompanies = $input.all();\n\n// Create a single summary item\nreturn [{\n  json: {\n    // Workflow identifier for email query\n    workflow_run_id: $execution.id,\n    \n    // Summary statistics\n    companies_processed: allCompanies.length,\n    \n    // Profile info (from first company - all have same profile)\n    profile_id: allCompanies[0]?.json.profile_id || 'unknown',\n    profile_name: allCompanies[0]?.json.profile_name || 'Unknown',\n    \n    // Timestamp\n    query_timestamp: new Date().toISOString()\n  }\n}];\n\n// WHAT GETS OUTPUT:\n// Single item with:\n// - workflow_run_id: Use this to query email queue\n// - companies_processed: How many companies we checked\n// - profile_id: Which profile was used for this run\n// - query_timestamp: When this summary was created\n\n// WHY THIS MATTERS:\n// If we didn't consolidate:\n// - Email workflow would execute MULTIPLE times (once per company)\n// - Each execution would return the SAME jobs from database\n// - User would receive duplicate emails\n\n// With consolidation:\n// - Email workflow executes ONCE\n// - Returns all jobs from this workflow run\n// - User receives single consolidated email\n\n// EXAMPLE:\n// Input: 40 items from Loop Over Companies Done branch (one per company)\n// Output: 1 item with workflow_run_id and summary\n// Email workflow executes: 1 time (not 40 times)\n\n// PROFILE CONTEXT:\n// We extract profile_id from the first company item. This is safe because\n// all companies in a single workflow run use the same profile (they were\n// merged with the same profile at the start). This helps the email workflow\n// know which profile generated these jobs.\n\n// Next: This would typically call the Send Email workflow\n//       (Not connected in this workflow, but would be in production)\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        0,
        0
      ],
      "id": "7d62b2f0-7861-4d88-899d-9957eb85a9dc",
      "name": "Workflow Summary"
    }
  ],
  "pinData": {},
  "connections": {
    "Loop Over Companies": {
      "main": [
        [
          {
            "node": "Workflow Summary",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Run Apify",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Run Apify": {
      "main": [
        [
          {
            "node": "If",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Log Error",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "If": {
      "main": [
        [
          {
            "node": "Add Context",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Log no results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Add Context": {
      "main": [
        [
          {
            "node": "Save Apify data",
            "type": "main",
            "index": 0
          },
          {
            "node": "Call 'Loop Jobs v3.1'",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Save Apify data": {
      "main": [
        [
          {
            "node": "Insert row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log no results": {
      "main": [
        [
          {
            "node": "Insert bad row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Error": {
      "main": [
        [
          {
            "node": "Insert bad row",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert bad row": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Start": {
      "main": [
        [
          {
            "node": "Load Companies",
            "type": "main",
            "index": 0
          },
          {
            "node": "Merge Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Load Companies": {
      "main": [
        [
          {
            "node": "Merge Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Request": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Data": {
      "main": [
        [
          {
            "node": "Build Request",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call 'Loop Jobs v3.1'": {
      "main": [
        [
          {
            "node": "Loop Over Companies",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "7a3bac5c-182e-47b4-bc20-d063d99df7fc",
  "meta": {
    "instanceId": "7534cf3caceb89a410b24188fae76f3b891a0be6d8178ba8a99d892ed4a4777d"
  },
  "id": "Wj0kFKBjCECg1MBw",
  "tags": []
}
